#!/usr/bin/env python3
"""
æ¼”ç®—æ³•åƒæ•¸å¯¦éš›ä½¿ç”¨ç¤ºä¾‹
Algorithm Parameters Usage Examples

é€™å€‹è…³æœ¬å±•ç¤ºå¦‚ä½•åœ¨æ‚¨çš„è»Šè¼›å°èˆªç³»çµ±ä¸­ç²å–ã€ä½¿ç”¨å’Œèª¿æ•´å„ç¨®æ¼”ç®—æ³•åƒæ•¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from algorithm.agent import QLearningAgent
from module.urban_grid import UrbanGrid
from vehicle import Vehicle
from algorithm.reward_config import RewardConfig

def demonstrate_parameter_sources():
    """æ¼”ç¤ºå„ç¨®åƒæ•¸çš„ä¾†æºå’Œä½¿ç”¨æ–¹æ³•"""
    print("ğŸ”§ æ¼”ç®—æ³•åƒæ•¸ä¾†æºå’Œä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. å‰µå»ºåŸºç¤ç’°å¢ƒ
    print("\nğŸ“ 1. ç’°å¢ƒå’ŒAgentåƒæ•¸")
    print("-" * 30)
    
    grid = UrbanGrid(size=20)
    agent = QLearningAgent(
        urban_grid=grid,
        learning_rate=0.2,    # Î± - å­¸ç¿’ç‡
        discount_factor=0.95, # Î³ - æŠ˜æ‰£å› å­  
        epsilon=0.2           # Îµ - æ¢ç´¢ç‡
    )
    
    print(f"ğŸ§  Agentåƒæ•¸ä¾†æº (agent.py):")
    print(f"   Î± (å­¸ç¿’ç‡): {agent.learning_rate}")
    print(f"   Î³ (æŠ˜æ‰£å› å­): {agent.discount_factor}")
    print(f"   Îµ (æ¢ç´¢ç‡): {agent.epsilon}")
    print(f"   å‹•ä½œç©ºé–“: {agent.actions}")
    print(f"   Qè¡¨å¤§å°: {len(agent.q_table)} å€‹ç‹€æ…‹")
    
    # 2. çå‹µé…ç½®åƒæ•¸
    print("\nğŸ’° 2. çå‹µé…ç½®åƒæ•¸")
    print("-" * 30)
    
    reward_config = RewardConfig()
    print(f"ğŸ“Š çå‹µåƒæ•¸ä¾†æº (reward_config.py):")
    print(f"   æ¼”ç®—æ³•é¡å‹: {reward_config.algorithm}")
    print(f"   æ­¥æ•¸æ‡²ç½°: {reward_config.step_penalty}")
    print(f"   åˆ°é”çå‹µ: {reward_config.destination_reached_reward}")
    print(f"   A*è·Ÿéš¨çå‹µ: {reward_config.astar_follow_reward}")
    print(f"   è·é›¢æ”¹å–„çå‹µ: {reward_config.closer_to_destination_reward}")
    
    # æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•å°ˆç”¨åƒæ•¸
    print(f"\nğŸ“ˆ æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•åƒæ•¸:")
    print(f"   åŸºç¤çå‹µ: {reward_config.exp_base_reward}")
    print(f"   æŒ¯å¹…: {reward_config.exp_amplitude}")
    print(f"   Xç¸®æ”¾: {reward_config.exp_x_scale}")
    print(f"   Yç¸®æ”¾: {reward_config.exp_y_scale}")
    
    # 3. è»Šè¼›å’Œç‹€æ…‹åƒæ•¸
    print("\nğŸš— 3. è»Šè¼›ç‹€æ…‹åƒæ•¸")
    print("-" * 30)
    
    vehicle = Vehicle(
        urban_grid=grid,
        agent=agent,
        position=(5, 5),
        destination=(15, 15),
        reward_config=reward_config
    )
    
    print(f"ğŸ“ è»Šè¼›ç‹€æ…‹ä¾†æº (vehicle.py):")
    print(f"   ç•¶å‰ä½ç½®: {vehicle.position}")
    print(f"   ç›®æ¨™ä½ç½®: {vehicle.destination}")
    print(f"   æœ€å„ªè·¯å¾‘é•·åº¦: {len(vehicle.optimal_path) if vehicle.optimal_path else 0}")
    print(f"   ç¸½æ­¥æ•¸: {vehicle.steps}")
    print(f"   ç¸½çå‹µ: {vehicle.total_reward}")
    
    # 4. æ¼”ç¤ºç‹€æ…‹å’Œå‹•ä½œçš„è¨ˆç®—
    print("\nğŸ¯ 4. ç‹€æ…‹å’Œå‹•ä½œè¨ˆç®—éç¨‹")
    print("-" * 30)
    
    # ç²å–ç•¶å‰ç‹€æ…‹
    congestion_level = grid.get_congestion_window(vehicle.position[0], vehicle.position[1])
    state = agent.get_state_key(vehicle.position, congestion_level)
    
    print(f"ğŸ” ç‹€æ…‹è¨ˆç®—:")
    print(f"   ä½ç½®: {vehicle.position}")
    print(f"   æ“å¡åº¦: {congestion_level:.3f}")
    print(f"   ç‹€æ…‹Key: '{state}'")
    
    # é¸æ“‡å‹•ä½œ
    action_idx = agent.choose_action(state, vehicle.position)
    dx, dy = agent.actions[action_idx]
    new_position = (vehicle.position[0] + dx, vehicle.position[1] + dy)
    
    print(f"\nğŸ® å‹•ä½œé¸æ“‡:")
    print(f"   å‹•ä½œç´¢å¼•: {action_idx}")
    print(f"   å‹•ä½œå‘é‡: ({dx}, {dy})")
    print(f"   æ–°ä½ç½®: {new_position}")
    
    # 5. çå‹µè¨ˆç®—ç¤ºä¾‹
    print("\nğŸ’ 5. çå‹µè¨ˆç®—è©³è§£")
    print("-" * 30)
    
    reward = vehicle.calculate_reward(new_position, dx, dy)
    print(f"ğŸ† çå‹µè¨ˆç®—çµæœ: {reward:.2f}")
    
    print(f"\nğŸ“‹ çå‹µçµ„æˆåˆ†æ:")
    print(f"   åŸºç¤æ­¥æ•¸æ‡²ç½°: {reward_config.step_penalty}")
    
    # æª¢æŸ¥æ˜¯å¦è·Ÿéš¨A*è·¯å¾‘
    if vehicle.optimal_path and len(vehicle.optimal_path) > 1:
        next_optimal = vehicle.optimal_path[1]
        if new_position == next_optimal:
            print(f"   A*è·¯å¾‘è·Ÿéš¨çå‹µ: +{reward_config.astar_follow_reward}")
        elif new_position in vehicle.optimal_path:
            print(f"   A*è·¯å¾‘ä¸Šçå‹µ: +{reward_config.astar_on_path_reward}")
    
    # æª¢æŸ¥è·é›¢æ”¹å–„
    from algorithm.astar import manhattan_distance
    current_dist = manhattan_distance(vehicle.position, vehicle.destination)
    new_dist = manhattan_distance(new_position, vehicle.destination)
    if new_dist < current_dist:
        print(f"   è·é›¢æ”¹å–„çå‹µ: +{reward_config.closer_to_destination_reward}")
    
    return agent, vehicle, reward_config

def demonstrate_parameter_adjustment():
    """æ¼”ç¤ºå¦‚ä½•èª¿æ•´åƒæ•¸"""
    print("\n\nâš™ï¸ åƒæ•¸èª¿æ•´ç¤ºä¾‹")
    print("=" * 60)
    
    grid = UrbanGrid(size=15)
    
    # 1. å‰µå»ºä¸åŒé…ç½®çš„Agent
    print("\nğŸ”§ 1. Agentåƒæ•¸èª¿æ•´")
    print("-" * 30)
    
    configs = [
        {"name": "ä¿å®ˆå‹", "lr": 0.1, "df": 0.99, "eps": 0.05},
        {"name": "å¹³è¡¡å‹", "lr": 0.2, "df": 0.95, "eps": 0.20},
        {"name": "æ¿€é€²å‹", "lr": 0.4, "df": 0.90, "eps": 0.50}
    ]
    
    for config in configs:
        agent = QLearningAgent(
            urban_grid=grid,
            learning_rate=config["lr"],
            discount_factor=config["df"],
            epsilon=config["eps"]
        )
        print(f"ğŸ“Š {config['name']}é…ç½®:")
        print(f"   å­¸ç¿’ç‡: {agent.learning_rate} (å­¸ç¿’{'å¿«' if agent.learning_rate > 0.3 else 'æ…¢'})")
        print(f"   æŠ˜æ‰£å› å­: {agent.discount_factor} ({'é‡è¦–é•·æœŸ' if agent.discount_factor > 0.95 else 'é‡è¦–çŸ­æœŸ'})")
        print(f"   æ¢ç´¢ç‡: {agent.epsilon} ({'é«˜æ¢ç´¢' if agent.epsilon > 0.3 else 'ä½æ¢ç´¢'})")
    
    # 2. çå‹µé…ç½®èª¿æ•´
    print("\nğŸ’° 2. çå‹µé…ç½®èª¿æ•´")
    print("-" * 30)
    
    reward_configs = [
        {
            "name": "è¬¹æ…å°èˆª",
            "step_penalty": -0.5,
            "destination_reward": 50,
            "astar_follow": 5
        },
        {
            "name": "æ¨™æº–å°èˆª", 
            "step_penalty": -1,
            "destination_reward": 100,
            "astar_follow": 10
        },
        {
            "name": "æ¿€é€²å°èˆª",
            "step_penalty": -2,
            "destination_reward": 200,
            "astar_follow": 20
        }
    ]
    
    for config in reward_configs:
        reward_config = RewardConfig()
        reward_config.update_config(
            step_penalty=config["step_penalty"],
            destination_reached_reward=config["destination_reward"],
            astar_follow_reward=config["astar_follow"]
        )
        
        print(f"ğŸ¯ {config['name']}:")
        print(f"   æ­¥æ•¸æˆæœ¬: {reward_config.step_penalty}")
        print(f"   åˆ°é”çå‹µ: {reward_config.destination_reached_reward}")
        print(f"   è·¯å¾‘è·Ÿéš¨: {reward_config.astar_follow_reward}")
    
    # 3. æ¼”ç®—æ³•åˆ‡æ›ç¤ºä¾‹
    print("\nğŸ”„ 3. æ¼”ç®—æ³•åˆ‡æ›")
    print("-" * 30)
    
    algorithms = ["proximity_based", "exponential_distance"]
    
    for algo in algorithms:
        reward_config = RewardConfig()
        reward_config.set_algorithm_type(algo)
        
        print(f"ğŸ“ˆ {algo.replace('_', ' ').title()}:")
        print(f"   æ¼”ç®—æ³•é¡å‹: {reward_config.get_algorithm_type()}")
        
        if algo == "exponential_distance":
            exp_config = reward_config.get_exponential_distance_config()
            print(f"   æŒ‡æ•¸åƒæ•¸: åŸºç¤={exp_config['base_reward']}, "
                  f"æŒ¯å¹…={exp_config['multiplier']}, "
                  f"ç¸®æ”¾=({exp_config['x_scale']}, {exp_config['y_scale']})")

def demonstrate_q_learning_update():
    """æ¼”ç¤ºQ-Learningæ›´æ–°éç¨‹"""
    print("\n\nğŸ§  Q-Learningæ›´æ–°éç¨‹è©³è§£")
    print("=" * 60)
    
    grid = UrbanGrid(size=10)
    agent = QLearningAgent(grid)
    
    # æ¨¡æ“¬ä¸€æ¬¡Qå€¼æ›´æ–°
    state = "5_5_0.20"  # ä½ç½®(5,5)ï¼Œæ“å¡åº¦0.20
    action = 1  # å‘å³ç§»å‹•
    reward = 8.5  # ç²å¾—çš„çå‹µ
    next_state = "6_5_0.15"  # æ–°ä½ç½®(6,5)ï¼Œæ“å¡åº¦0.15
    
    print(f"ğŸ“ æ›´æ–°å‰ç‹€æ…‹:")
    print(f"   ç•¶å‰ç‹€æ…‹: {state}")
    print(f"   åŸ·è¡Œå‹•ä½œ: {action} ({['ä¸Š','å³','ä¸‹','å·¦'][action]})")
    print(f"   ç²å¾—çå‹µ: {reward}")
    print(f"   ä¸‹ä¸€ç‹€æ…‹: {next_state}")
    
    # ç²å–æ›´æ–°å‰çš„Qå€¼
    old_q_value = agent.q_table[state][action]
    max_next_q = max(agent.q_table[next_state])
    
    print(f"\nğŸ”¢ Qå€¼è¨ˆç®—:")
    print(f"   èˆŠQå€¼ Q(s,a): {old_q_value:.3f}")
    print(f"   æœ€å¤§ä¸‹ä¸€Qå€¼ max Q(s',a'): {max_next_q:.3f}")
    print(f"   å­¸ç¿’ç‡ Î±: {agent.learning_rate}")
    print(f"   æŠ˜æ‰£å› å­ Î³: {agent.discount_factor}")
    
    # è¨ˆç®—ç›®æ¨™å€¼
    target = reward + agent.discount_factor * max_next_q
    td_error = target - old_q_value
    
    print(f"\nğŸ“Š è¨ˆç®—éç¨‹:")
    print(f"   ç›®æ¨™å€¼ = r + Î³ Ã— max Q(s',a')")
    print(f"          = {reward} + {agent.discount_factor} Ã— {max_next_q:.3f}")
    print(f"          = {target:.3f}")
    print(f"   TDèª¤å·® = ç›®æ¨™å€¼ - èˆŠQå€¼")
    print(f"          = {target:.3f} - {old_q_value:.3f}")
    print(f"          = {td_error:.3f}")
    
    # åŸ·è¡ŒQå€¼æ›´æ–°
    agent.update_q_table(state, action, reward, next_state)
    new_q_value = agent.q_table[state][action]
    
    print(f"\nâœ… æ›´æ–°çµæœ:")
    print(f"   æ–°Qå€¼ = èˆŠQå€¼ + Î± Ã— TDèª¤å·®")
    print(f"         = {old_q_value:.3f} + {agent.learning_rate} Ã— {td_error:.3f}")
    print(f"         = {new_q_value:.3f}")
    print(f"   Qå€¼è®ŠåŒ–: {new_q_value - old_q_value:+.3f}")

def main():
    """ä¸»å‡½æ•¸ - åŸ·è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš— è»Šè¼›å°èˆªç³»çµ±åƒæ•¸è©³è§£æ¼”ç¤º")
    print("=" * 80)
    
    try:
        # åŸºæœ¬åƒæ•¸æ¼”ç¤º
        agent, vehicle, reward_config = demonstrate_parameter_sources()
        
        # åƒæ•¸èª¿æ•´æ¼”ç¤º
        demonstrate_parameter_adjustment()
        
        # Q-Learningæ›´æ–°æ¼”ç¤º
        demonstrate_q_learning_update()
        
        print(f"\n\nğŸ’¡ ç¸½çµ")
        print("=" * 80)
        print("ğŸ“– åƒæ•¸ä¾†æºå°ç…§:")
        print("   â€¢ Î±, Î³, Îµ â†’ agent.py (QLearningAgenté¡)")
        print("   â€¢ çå‹µåƒæ•¸ â†’ reward_config.py (RewardConfigé¡)")
        print("   â€¢ ç‹€æ…‹ s â†’ vehicle.py (ä½ç½®+æ“å¡åº¦)")
        print("   â€¢ å‹•ä½œ a â†’ agent.py (0=ä¸Š, 1=å³, 2=ä¸‹, 3=å·¦)")
        print("   â€¢ çå‹µ r â†’ vehicle.calculate_reward()")
        print("")
        print("ğŸ”§ èª¿æ•´æ–¹æ³•:")
        print("   â€¢ GUI: æ¨¡æ“¬æ§åˆ¶å™¨ä¸­çš„åƒæ•¸è¼¸å…¥æ¡†")
        print("   â€¢ ç¨‹å¼: å‰µå»ºAgentå’ŒRewardConfigæ™‚æŒ‡å®š")
        print("   â€¢ å‹•æ…‹: é‹è¡Œæ™‚ä¿®æ”¹agent.learning_rateç­‰å±¬æ€§")
        print("")
        print("ğŸ¯ é€™äº›åƒæ•¸æ§åˆ¶äº†è»Šè¼›çš„å­¸ç¿’è¡Œç‚ºå’Œå°èˆªç­–ç•¥ï¼")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {str(e)}")
        print("è«‹ç¢ºä¿æ‰€æœ‰å¿…è¦çš„æ¨¡çµ„éƒ½å·²æ­£ç¢ºå°å…¥")
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
