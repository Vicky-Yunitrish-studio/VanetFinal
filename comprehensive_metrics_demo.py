#!/usr/bin/env python3
"""
å®Œæ•´çš„äº”å€‹æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™è¨ˆç®—å’Œåˆ†ææ¼”ç¤º
Comprehensive Five Core Performance Metrics Calculation and Analysis Demo

é€™å€‹è…³æœ¬å±•ç¤ºå¦‚ä½•è¨ˆç®—å’Œåˆ†æå°èˆªç³»çµ±çš„äº”å€‹æ ¸å¿ƒè©•ä¼°æŒ‡æ¨™ï¼š
1. æˆåŠŸç‡ (Success Rate)
2. å¹³å‡æ­¥æ•¸ (Average Steps)  
3. è·¯å¾‘æ•ˆç‡ (Path Efficiency)
4. è¨ˆç®—æ™‚é–“ (Computation Time)
5. å¹³å‡çå‹µ (Average Reward)
"""

import time
import random
import numpy as np
from typing import Dict, List, Tuple, NamedTuple
import matplotlib.pyplot as plt
import sys
import os

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘ä»¥ä¾¿å°å…¥æ¨¡çµ„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class NavigationResult(NamedTuple):
    """å°èˆªçµæœæ•¸æ“šçµæ§‹"""
    success: bool
    steps: int
    total_reward: float
    path: List[Tuple[int, int]]
    start: Tuple[int, int]
    end: Tuple[int, int]
    computation_time: float  # æ¯«ç§’

class MetricsCalculator:
    """äº”å€‹æ ¸å¿ƒæŒ‡æ¨™çš„è¨ˆç®—å™¨"""
    
    def __init__(self):
        self.results: List[NavigationResult] = []
    
    def add_result(self, result: NavigationResult):
        """æ·»åŠ ä¸€æ¬¡å°èˆªçµæœ"""
        self.results.append(result)
    
    def calculate_astar_optimal_steps(self, start: Tuple[int, int], end: Tuple[int, int]) -> int:
        """è¨ˆç®—A*æ¼”ç®—æ³•çš„ç†è«–æœ€çŸ­æ­¥æ•¸ï¼ˆæ›¼å“ˆé “è·é›¢ï¼‰"""
        return abs(start[0] - end[0]) + abs(start[1] - end[1])
    
    def get_metrics(self) -> Dict[str, float]:
        """è¨ˆç®—æ‰€æœ‰äº”å€‹æ ¸å¿ƒæŒ‡æ¨™"""
        if not self.results:
            return {
                'success_rate': 0.0,
                'avg_steps': 0.0,
                'path_efficiency': 0.0,
                'avg_computation_time': 0.0,
                'avg_reward': 0.0
            }
        
        # 1. æˆåŠŸç‡ (Success Rate)
        successful_results = [r for r in self.results if r.success]
        success_rate = len(successful_results) / len(self.results) * 100
        
        if not successful_results:
            return {
                'success_rate': success_rate,
                'avg_steps': 0.0,
                'path_efficiency': 0.0,
                'avg_computation_time': 0.0,
                'avg_reward': 0.0
            }
        
        # 2. å¹³å‡æ­¥æ•¸ (Average Steps) - åªè¨ˆç®—æˆåŠŸçš„ä»»å‹™
        avg_steps = sum(r.steps for r in successful_results) / len(successful_results)
        
        # 3. è·¯å¾‘æ•ˆç‡ (Path Efficiency)
        path_efficiencies = []
        for result in successful_results:
            optimal_steps = self.calculate_astar_optimal_steps(result.start, result.end)
            efficiency = optimal_steps / result.steps if result.steps > 0 else 0
            path_efficiencies.append(efficiency)
        
        avg_path_efficiency = sum(path_efficiencies) / len(path_efficiencies) * 100 if path_efficiencies else 0
        
        # 4. è¨ˆç®—æ™‚é–“ (Computation Time)
        avg_computation_time = sum(r.computation_time for r in self.results) / len(self.results)
        
        # 5. å¹³å‡çå‹µ (Average Reward) - æ¯æ­¥çš„å¹³å‡çå‹µ
        total_reward = sum(r.total_reward for r in successful_results)
        total_steps = sum(r.steps for r in successful_results)
        avg_reward = total_reward / total_steps if total_steps > 0 else 0
        
        return {
            'success_rate': success_rate,
            'avg_steps': avg_steps,
            'path_efficiency': avg_path_efficiency,
            'avg_computation_time': avg_computation_time,
            'avg_reward': avg_reward
        }
    
    def get_detailed_analysis(self) -> Dict:
        """ç²å–è©³ç´°çš„åˆ†æå ±å‘Š"""
        metrics = self.get_metrics()
        successful_results = [r for r in self.results if r.success]
        
        analysis = {
            'basic_metrics': metrics,
            'test_summary': {
                'total_tests': len(self.results),
                'successful_tests': len(successful_results),
                'failed_tests': len(self.results) - len(successful_results)
            },
            'performance_evaluation': self._evaluate_performance(metrics),
            'detailed_stats': self._get_detailed_stats(successful_results)
        }
        
        return analysis
    
    def _evaluate_performance(self, metrics: Dict[str, float]) -> Dict[str, str]:
        """æ ¹æ“šæŒ‡æ¨™è©•ä¼°æ€§èƒ½ç­‰ç´š"""
        evaluation = {}
        
        # æˆåŠŸç‡è©•ä¼°
        if metrics['success_rate'] >= 95:
            evaluation['success_rate'] = 'å„ªç§€ (Excellent)'
        elif metrics['success_rate'] >= 85:
            evaluation['success_rate'] = 'è‰¯å¥½ (Good)'
        elif metrics['success_rate'] >= 70:
            evaluation['success_rate'] = 'ä¸€èˆ¬ (Fair)'
        else:
            evaluation['success_rate'] = 'éœ€æ”¹é€² (Needs Improvement)'
        
        # è·¯å¾‘æ•ˆç‡è©•ä¼°
        if metrics['path_efficiency'] >= 80:
            evaluation['path_efficiency'] = 'é«˜æ•ˆç‡ (High Efficiency)'
        elif metrics['path_efficiency'] >= 60:
            evaluation['path_efficiency'] = 'ä¸­ç­‰æ•ˆç‡ (Medium Efficiency)'
        else:
            evaluation['path_efficiency'] = 'ä½æ•ˆç‡ (Low Efficiency)'
        
        # è¨ˆç®—æ™‚é–“è©•ä¼°
        if metrics['avg_computation_time'] < 1:
            evaluation['computation_time'] = 'å„ªç§€ (Excellent - Real-time)'
        elif metrics['avg_computation_time'] < 5:
            evaluation['computation_time'] = 'è‰¯å¥½ (Good - Fast)'
        elif metrics['avg_computation_time'] < 20:
            evaluation['computation_time'] = 'ä¸€èˆ¬ (Fair - Acceptable)'
        else:
            evaluation['computation_time'] = 'éœ€æ”¹é€² (Needs Improvement)'
        
        # å¹³å‡çå‹µè©•ä¼°
        if metrics['avg_reward'] > 5:
            evaluation['avg_reward'] = 'å„ªç§€ (Excellent)'
        elif metrics['avg_reward'] > 2:
            evaluation['avg_reward'] = 'è‰¯å¥½ (Good)'
        elif metrics['avg_reward'] > 0:
            evaluation['avg_reward'] = 'ä¸€èˆ¬ (Fair)'
        else:
            evaluation['avg_reward'] = 'éœ€æ”¹é€² (Needs Improvement)'
        
        return evaluation
    
    def _get_detailed_stats(self, successful_results: List[NavigationResult]) -> Dict:
        """ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯"""
        if not successful_results:
            return {}
        
        steps = [r.steps for r in successful_results]
        rewards = [r.total_reward for r in successful_results]
        computation_times = [r.computation_time for r in self.results]
        
        return {
            'steps_stats': {
                'min': min(steps),
                'max': max(steps),
                'std': np.std(steps),
                'median': np.median(steps)
            },
            'reward_stats': {
                'min': min(rewards),
                'max': max(rewards),
                'std': np.std(rewards),
                'median': np.median(rewards)
            },
            'computation_time_stats': {
                'min': min(computation_times),
                'max': max(computation_times),
                'std': np.std(computation_times),
                'median': np.median(computation_times)
            }
        }

def simulate_navigation_algorithm(algorithm_name: str, start: Tuple[int, int], 
                                end: Tuple[int, int]) -> NavigationResult:
    """æ¨¡æ“¬å°èˆªæ¼”ç®—æ³•åŸ·è¡Œ"""
    start_time = time.time()
    
    # æ¨¡æ“¬ä¸åŒæ¼”ç®—æ³•çš„ç‰¹æ€§
    if algorithm_name == "proximity":
        # é„°è¿‘æ€§æ¼”ç®—æ³•ï¼šè¼ƒç©©å®šä½†å¯èƒ½æ­¥æ•¸è¼ƒå¤š
        success_prob = 0.95
        base_steps = abs(start[0] - end[0]) + abs(start[1] - end[1])  # æ›¼å“ˆé “è·é›¢
        steps_multiplier = random.uniform(1.2, 1.8)  # æ¯”æœ€å„ªè·¯å¾‘å¤š20%-80%
        reward_per_step = random.uniform(3, 6)  # ç©©å®šçš„çå‹µ
        computation_delay = random.uniform(0.3, 0.8)  # å¿«é€Ÿè¨ˆç®—
        
    elif algorithm_name == "exponential":
        # æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•ï¼šæ›´ç²¾ç¢ºä½†è¨ˆç®—è¼ƒæ…¢
        success_prob = 0.88
        base_steps = abs(start[0] - end[0]) + abs(start[1] - end[1])
        steps_multiplier = random.uniform(1.1, 1.5)  # æ¯”æœ€å„ªè·¯å¾‘å¤š10%-50%
        reward_per_step = random.uniform(-1, 8)  # è®ŠåŒ–è¼ƒå¤§çš„çå‹µ
        computation_delay = random.uniform(0.8, 2.5)  # è¼ƒæ…¢çš„è¨ˆç®—
        
    else:
        raise ValueError(f"Unknown algorithm: {algorithm_name}")
    
    # æ¨¡æ“¬è¨ˆç®—æ™‚é–“
    time.sleep(computation_delay / 1000)  # è½‰æ›ç‚ºç§’
    end_time = time.time()
    computation_time = (end_time - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
    
    # æ±ºå®šæ˜¯å¦æˆåŠŸ
    success = random.random() < success_prob
    
    if success:
        steps = int(base_steps * steps_multiplier)
        total_reward = steps * reward_per_step + random.uniform(50, 100)  # é¡å¤–çš„åˆ°é”çå‹µ
        # ç”Ÿæˆç°¡åŒ–çš„è·¯å¾‘
        path = [(start[0] + i, start[1]) for i in range(steps//2)] + \
               [(end[0], start[1] + i) for i in range(steps//2)]
    else:
        steps = 0
        total_reward = -50  # å¤±æ•—æ‡²ç½°
        path = []
    
    return NavigationResult(
        success=success,
        steps=steps,
        total_reward=total_reward,
        path=path,
        start=start,
        end=end,
        computation_time=computation_time
    )

def run_comprehensive_evaluation():
    """åŸ·è¡Œå®Œæ•´çš„äº”å€‹æŒ‡æ¨™è©•ä¼°"""
    print("ğŸš— å°èˆªç³»çµ±äº”å€‹æ ¸å¿ƒæŒ‡æ¨™è©•ä¼°æ¼”ç¤º")
    print("=" * 60)
    
    # æ¸¬è©¦å…©ç¨®æ¼”ç®—æ³•
    algorithms = ["proximity", "exponential"]
    test_cases = 50  # æ¯å€‹æ¼”ç®—æ³•æ¸¬è©¦50æ¬¡
    
    results = {}
    
    for algorithm in algorithms:
        print(f"\nğŸ“Š è©•ä¼°æ¼”ç®—æ³•: {algorithm.upper()}")
        print("-" * 40)
        
        calculator = MetricsCalculator()
        
        # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦
        for i in range(test_cases):
            # éš¨æ©Ÿç”Ÿæˆèµ·é»å’Œçµ‚é»
            start = (random.randint(0, 10), random.randint(0, 10))
            end = (random.randint(0, 10), random.randint(0, 10))
            
            # ç¢ºä¿èµ·é»å’Œçµ‚é»ä¸åŒ
            while start == end:
                end = (random.randint(0, 10), random.randint(0, 10))
            
            # åŸ·è¡Œå°èˆªæ¨¡æ“¬
            result = simulate_navigation_algorithm(algorithm, start, end)
            calculator.add_result(result)
            
            # é¡¯ç¤ºé€²åº¦
            if (i + 1) % 10 == 0:
                print(f"  å·²å®Œæˆ {i + 1}/{test_cases} æ¬¡æ¸¬è©¦...")
        
        # è¨ˆç®—ä¸¦é¡¯ç¤ºçµæœ
        analysis = calculator.get_detailed_analysis()
        results[algorithm] = analysis
        
        print("\nğŸ“ˆ æ ¸å¿ƒæŒ‡æ¨™çµæœ:")
        metrics = analysis['basic_metrics']
        evaluation = analysis['performance_evaluation']
        
        print(f"  1ï¸âƒ£  æˆåŠŸç‡: {metrics['success_rate']:.1f}% - {evaluation['success_rate']}")
        print(f"  2ï¸âƒ£  å¹³å‡æ­¥æ•¸: {metrics['avg_steps']:.1f} æ­¥")
        print(f"  3ï¸âƒ£  è·¯å¾‘æ•ˆç‡: {metrics['path_efficiency']:.1f}% - {evaluation['path_efficiency']}")
        print(f"  4ï¸âƒ£  è¨ˆç®—æ™‚é–“: {metrics['avg_computation_time']:.2f} ms - {evaluation['computation_time']}")
        print(f"  5ï¸âƒ£  å¹³å‡çå‹µ: {metrics['avg_reward']:.2f} åˆ†/æ­¥ - {evaluation['avg_reward']}")
        
        # é¡¯ç¤ºæ¸¬è©¦çµ±è¨ˆ
        test_summary = analysis['test_summary']
        print(f"\nğŸ“‹ æ¸¬è©¦çµ±è¨ˆ:")
        print(f"  ç¸½æ¸¬è©¦æ¬¡æ•¸: {test_summary['total_tests']}")
        print(f"  æˆåŠŸæ¬¡æ•¸: {test_summary['successful_tests']}")
        print(f"  å¤±æ•—æ¬¡æ•¸: {test_summary['failed_tests']}")
    
    # æ¼”ç®—æ³•æ¯”è¼ƒ
    print("\nğŸ”„ æ¼”ç®—æ³•æ¯”è¼ƒåˆ†æ")
    print("=" * 60)
    
    proximity_metrics = results['proximity']['basic_metrics']
    exponential_metrics = results['exponential']['basic_metrics']
    
    print(f"{'æŒ‡æ¨™':<15} {'é„°è¿‘æ€§æ¼”ç®—æ³•':<15} {'æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•':<15} {'è¼ƒå„ªè€…':<10}")
    print("-" * 65)
    
    comparisons = [
        ('æˆåŠŸç‡ (%)', 'success_rate', True),
        ('å¹³å‡æ­¥æ•¸', 'avg_steps', False),
        ('è·¯å¾‘æ•ˆç‡ (%)', 'path_efficiency', True),
        ('è¨ˆç®—æ™‚é–“ (ms)', 'avg_computation_time', False),
        ('å¹³å‡çå‹µ', 'avg_reward', True)
    ]
    
    for name, key, higher_is_better in comparisons:
        prox_val = proximity_metrics[key]
        exp_val = exponential_metrics[key]
        
        if higher_is_better:
            winner = "é„°è¿‘æ€§" if prox_val > exp_val else "æŒ‡æ•¸è·é›¢"
        else:
            winner = "é„°è¿‘æ€§" if prox_val < exp_val else "æŒ‡æ•¸è·é›¢"
        
        print(f"{name:<15} {prox_val:<15.2f} {exp_val:<15.2f} {winner:<10}")
    
    # ç¶œåˆè©•åˆ†
    print("\nğŸ† ç¶œåˆè©•åˆ†")
    print("-" * 30)
    
    def calculate_comprehensive_score(metrics):
        """è¨ˆç®—ç¶œåˆè©•åˆ†"""
        # æ­£è¦åŒ–å„æŒ‡æ¨™
        success_rate_norm = metrics['success_rate'] / 100
        path_efficiency_norm = metrics['path_efficiency'] / 100
        steps_efficiency = min(1.0, 10 / max(metrics['avg_steps'], 1))
        time_efficiency = min(1.0, 1.0 / max(metrics['avg_computation_time'], 0.1))
        reward_efficiency = min(1.0, max(0, metrics['avg_reward'] / 8.0))
        
        # åŠ æ¬Šè¨ˆç®—
        score = (success_rate_norm * 0.35 + 
                path_efficiency_norm * 0.25 + 
                steps_efficiency * 0.20 + 
                time_efficiency * 0.10 + 
                reward_efficiency * 0.10)
        
        return score * 100
    
    prox_score = calculate_comprehensive_score(proximity_metrics)
    exp_score = calculate_comprehensive_score(exponential_metrics)
    
    print(f"é„°è¿‘æ€§æ¼”ç®—æ³•ç¶œåˆå¾—åˆ†: {prox_score:.1f}åˆ†")
    print(f"æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•ç¶œåˆå¾—åˆ†: {exp_score:.1f}åˆ†")
    
    winner = "é„°è¿‘æ€§æ¼”ç®—æ³•" if prox_score > exp_score else "æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•"
    print(f"\nğŸ¥‡ ç¸½é«”å„ªå‹è€…: {winner}")
    
    print("\nğŸ’¡ æŒ‡æ¨™åˆ†æå»ºè­°:")
    print("=" * 40)
    print("â€¢ æˆåŠŸç‡: åæ˜ æ¼”ç®—æ³•çš„å¯é æ€§ï¼Œè¶Šé«˜è¶Šå¥½")
    print("â€¢ å¹³å‡æ­¥æ•¸: åæ˜ è·¯å¾‘é•·åº¦ï¼Œè¶Šå°‘è¶Šå¥½")
    print("â€¢ è·¯å¾‘æ•ˆç‡: åæ˜ è·¯å¾‘å„ªåŒ–ç¨‹åº¦ï¼Œè¶Šé«˜è¶Šå¥½")
    print("â€¢ è¨ˆç®—æ™‚é–“: åæ˜ å¯¦æ™‚æ€§èƒ½ï¼Œè¶ŠçŸ­è¶Šå¥½")
    print("â€¢ å¹³å‡çå‹µ: åæ˜ è¡Œç‚ºå“è³ªï¼Œè¶Šé«˜è¶Šå¥½")
    print("\nğŸ¯ é¸æ“‡æ¼”ç®—æ³•æ™‚éœ€è¦æ ¹æ“šå…·é«”æ‡‰ç”¨å ´æ™¯è€ƒæ…®å„æŒ‡æ¨™çš„é‡è¦æ€§ï¼")

if __name__ == "__main__":
    run_comprehensive_evaluation()
