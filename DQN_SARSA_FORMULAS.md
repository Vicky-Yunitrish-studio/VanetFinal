# DQN & SARSA æ¼”ç®—æ³•å…¬å¼ç¸½æ•´ç†

## ğŸ“‹ å¿«é€Ÿå°æ¯”è¡¨

| æ¼”ç®—æ³• | æ›´æ–°å…¬å¼ | ç‰¹é» | é©ç”¨å ´æ™¯ |
|--------|----------|------|----------|
| **Q-Learning** | `Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]` | é›¢ç­–ç•¥ã€æœ€å„ªè§£ | æ‚¨ç›®å‰çš„ç³»çµ± âœ… |
| **SARSA** | `Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]` | åœ¨ç­–ç•¥ã€ä¿å®ˆå­¸ç¿’ | å®‰å…¨å°èˆª |
| **DQN** | `L(Î¸) = E[(r + Î³ max Q(s',a';Î¸â») - Q(s,a;Î¸))Â²]` | æ·±åº¦ç¶²çµ¡ã€å¤§ç‹€æ…‹ç©ºé–“ | è¤‡é›œç’°å¢ƒ |

---

## ğŸ§  Q-Learningï¼ˆæ‚¨ç•¶å‰ä½¿ç”¨çš„ï¼‰

### æ ¸å¿ƒå…¬å¼

```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
                            a'
```

### é—œéµæ¦‚å¿µ

- **é›¢ç­–ç•¥ (Off-policy)**ï¼šå­¸ç¿’æœ€å„ªç­–ç•¥ï¼Œä¸ç®¡ç•¶å‰åŸ·è¡Œä»€éº¼ç­–ç•¥
- **è²çˆ¾æ›¼æ–¹ç¨‹**ï¼šä½¿ç”¨ä¸‹ä¸€ç‹€æ…‹çš„æœ€å¤§Qå€¼æ›´æ–°
- **æ”¶æ–‚æ€§**ï¼šä¿è­‰æ”¶æ–‚åˆ°æœ€å„ªQ*å‡½æ•¸

### æ‚¨ç³»çµ±ä¸­çš„å¯¦ç¾

```python
# ä¾†è‡ª agent.py
def update_q_table(self, state, action, reward, next_state):
    best_next_action = np.argmax(self.q_table[next_state])
    self.q_table[state][action] = (1 - self.learning_rate) * self.q_table[state][action] + \
                                 self.learning_rate * (reward + self.discount_factor * 
                                                     self.q_table[next_state][best_next_action])
```

---

## ğŸ¯ SARSA æ¼”ç®—æ³•

### æ ¸å¿ƒå…¬å¼

```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```

### èˆ‡Q-Learningçš„å·®ç•°

| Q-Learning | SARSA |
|------------|-------|
| `max Q(s',a')` | `Q(s',a')` |
| ä½¿ç”¨æœ€å„ªå‹•ä½œ | ä½¿ç”¨å¯¦éš›é¸æ“‡çš„å‹•ä½œ |
| é›¢ç­–ç•¥å­¸ç¿’ | åœ¨ç­–ç•¥å­¸ç¿’ |

### å®Œæ•´æ¼”ç®—æ³•æµç¨‹

```
1. åˆå§‹åŒ– Q(s,a)
2. è§€å¯Ÿç‹€æ…‹ sï¼Œé¸æ“‡å‹•ä½œ a
3. åŸ·è¡Œå‹•ä½œ aï¼Œå¾—åˆ° r, s'
4. åœ¨ s' é¸æ“‡å‹•ä½œ a'
5. æ›´æ–° Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
6. s â† s', a â† a'
7. é‡è¤‡æ­¥é©Ÿ 3-6
```

### é©åˆè»Šè¼›å°èˆªçš„åŸå› 

- **å®‰å…¨å­¸ç¿’**ï¼šä¸æœƒå­¸ç¿’å±éšªçš„æ¢ç´¢å‹•ä½œ
- **ä¿å®ˆç­–ç•¥**ï¼šé¿å…åœ¨å­¸ç¿’éç¨‹ä¸­å†’éšª
- **å¯¦éš›å¯è¡Œ**ï¼šå­¸ç¿’ç•¶å‰ç­–ç•¥ä¸‹çš„æœ€ä½³è¡¨ç¾

### å¯¦ç¾ç¯„ä¾‹

```python
class SARSAAgent(QLearningAgent):
    def update_q_table(self, state, action, reward, next_state, next_action):
        """SARSAæ›´æ–°ï¼šä½¿ç”¨å¯¦éš›é¸æ“‡çš„next_action"""
        if next_action is not None:
            target = reward + self.discount_factor * self.q_table[next_state][next_action]
        else:
            target = reward  # çµ‚ç«¯ç‹€æ…‹
            
        self.q_table[state][action] = (1 - self.learning_rate) * self.q_table[state][action] + \
                                     self.learning_rate * target
```

---

## ğŸ”® Deep Q-Network (DQN)

### æ ¸å¿ƒæ¦‚å¿µ

DQN = Q-Learning + æ·±åº¦ç¥ç¶“ç¶²çµ¡

### æå¤±å‡½æ•¸

```
L(Î¸) = E[(y - Q(s,a;Î¸))Â²]

å…¶ä¸­: y = r + Î³ max Q(s',a';Î¸â»)
                a'
```

### é—œéµæŠ€è¡“

#### 1. ç¥ç¶“ç¶²çµ¡çµæ§‹

```
è¼¸å…¥å±¤: ç‹€æ…‹ç‰¹å¾µ
éš±è—å±¤: å…¨é€£æ¥å±¤ (64-128 ç¥ç¶“å…ƒ)
è¼¸å‡ºå±¤: 4å€‹å‹•ä½œçš„Qå€¼ [ä¸Š,å³,ä¸‹,å·¦]
```

#### 2. ç¶“é©—å›æ”¾ (Experience Replay)

```python
class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
```

#### 3. ç›®æ¨™ç¶²çµ¡ (Target Network)

```python
# æ¯Næ­¥æ›´æ–°ç›®æ¨™ç¶²çµ¡
if step % target_update_freq == 0:
    target_network.load_state_dict(main_network.state_dict())
```

### è»Šè¼›å°èˆªä¸­çš„æ‡‰ç”¨

#### ç‹€æ…‹è¡¨ç¤º

```python
def get_state_representation(vehicle_pos, destination, congestion_map):
    state = [
        vehicle_pos[0] / grid_size,      # æ­£è¦åŒ–xåº§æ¨™
        vehicle_pos[1] / grid_size,      # æ­£è¦åŒ–yåº§æ¨™
        destination[0] / grid_size,      # æ­£è¦åŒ–ç›®æ¨™x
        destination[1] / grid_size,      # æ­£è¦åŒ–ç›®æ¨™y
        *congestion_map.flatten()        # æ“å¡åœ°åœ–
    ]
    return np.array(state)
```

#### ç¶²çµ¡æ¶æ§‹ç¯„ä¾‹

```python
class DQNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

---

## âš–ï¸ ä¸‰ç¨®æ¼”ç®—æ³•å¯¦ç”¨æ¯”è¼ƒ

### åœ¨æ‚¨çš„è»Šè¼›å°èˆªç³»çµ±ä¸­çš„é¸æ“‡å»ºè­°

#### ğŸ¯ Q-Learningï¼ˆç•¶å‰ï¼‰

**ä½•æ™‚ä½¿ç”¨ï¼š**

- âœ… 20x20 ç¶²æ ¼ç’°å¢ƒï¼ˆå°ç‹€æ…‹ç©ºé–“ï¼‰
- âœ… å¿«é€ŸåŸå‹å’Œæ¸¬è©¦
- âœ… éœ€è¦é€æ˜çš„æ±ºç­–éç¨‹
- âœ… è¨ˆç®—è³‡æºæœ‰é™

**å„ªå‹¢ï¼š**

- å¯¦ç¾ç°¡å–®ï¼Œæ˜“æ–¼èª¿è©¦
- æ”¶æ–‚æœ‰ç†è«–ä¿è­‰
- å¯ç›´æ¥æŸ¥çœ‹Qè¡¨äº†è§£å­¸ç¿’çµæœ

#### ğŸ›¡ï¸ SARSAï¼ˆå»ºè­°æ“´å±•ï¼‰

**ä½•æ™‚ä½¿ç”¨ï¼š**

- ğŸš— å®‰å…¨é—œéµçš„è»Šè¼›å°èˆª
- ğŸ”’ ä¸èƒ½æ‰¿å—æ¢ç´¢é¢¨éšª
- ğŸ“ˆ åœ¨ç·šå­¸ç¿’ç’°å¢ƒ
- ğŸ¯ éœ€è¦ä¿å®ˆç­–ç•¥

**å¯¦ç¾å»ºè­°ï¼š**

```python
# åœ¨æ‚¨çš„ç³»çµ±ä¸­æ·»åŠ SARSAé¸é …
def create_agent(algorithm_type="qlearning"):
    if algorithm_type == "sarsa":
        return SARSAAgent(urban_grid)
    else:
        return QLearningAgent(urban_grid)
```

#### ğŸ”® DQNï¼ˆæœªä¾†æ“´å±•ï¼‰

**ä½•æ™‚ä½¿ç”¨ï¼š**

- ğŸ—ºï¸ æ›´å¤§çš„åœ°åœ–ï¼ˆ100x100+ï¼‰
- ğŸ–¼ï¸ éœ€è¦è™•ç†åœ–åƒè¼¸å…¥
- ğŸ§  è¤‡é›œçš„ç‹€æ…‹ç‰¹å¾µ
- ğŸ’ª æœ‰å……è¶³è¨ˆç®—è³‡æº

**æº–å‚™å·¥ä½œï¼š**

- éœ€è¦GPUæ”¯æ´
- éœ€è¦å¤§é‡è¨“ç·´æ•¸æ“š
- éœ€è¦æ›´è¤‡é›œçš„è¶…åƒæ•¸èª¿å„ª

---

## ğŸ”§ å¯¦éš›æ•´åˆå»ºè­°

### 1. æ“´å±•æ‚¨çš„agent.py

```python
class QLearningAgent:
    def __init__(self, urban_grid, algorithm="qlearning", ...):
        self.algorithm = algorithm
        # ... å…¶ä»–åˆå§‹åŒ–
    
    def update_q_table(self, state, action, reward, next_state, next_action=None):
        if self.algorithm == "sarsa" and next_action is not None:
            # SARSAæ›´æ–°
            target = reward + self.discount_factor * self.q_table[next_state][next_action]
        else:
            # Q-Learningæ›´æ–°
            target = reward + self.discount_factor * np.max(self.q_table[next_state])
        
        self.q_table[state][action] = (1 - self.learning_rate) * self.q_table[state][action] + \
                                     self.learning_rate * target
```

### 2. åœ¨GUIä¸­æ·»åŠ æ¼”ç®—æ³•é¸æ“‡

```python
# åœ¨simulation_controller.pyä¸­
algorithms = ["Q-Learning", "SARSA", "DQN"]
self.algorithm_combo = ttk.Combobox(parent, values=algorithms)
```

### 3. æ€§èƒ½æ¯”è¼ƒæ¡†æ¶

```python
def compare_algorithms():
    results = {}
    for algo in ["qlearning", "sarsa"]:
        agent = create_agent(algo)
        metrics = run_evaluation(agent, episodes=100)
        results[algo] = metrics
    return results
```

é€™æ¨£æ‚¨å°±æœ‰äº†å®Œæ•´çš„æ¼”ç®—æ³•å…¬å¼åƒè€ƒå’Œå¯¦éš›æ•´åˆå»ºè­°ï¼æ¯å€‹æ¼”ç®—æ³•éƒ½æœ‰å…¶é©ç”¨çš„å ´æ™¯ï¼Œå¯ä»¥æ ¹æ“šå…·é«”éœ€æ±‚é¸æ“‡æœ€åˆé©çš„æ–¹æ³•ã€‚
