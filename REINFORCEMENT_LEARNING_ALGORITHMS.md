# å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•å…¬å¼ç¸½æ•´ç† (Reinforcement Learning Algorithms Formulas)

## ğŸ“š ä¸‰å¤§æ ¸å¿ƒæ¼”ç®—æ³•æ¦‚è¦½

æœ¬æ–‡æª”æ•´ç†äº†åœ¨è»Šè¼›å°èˆªç³»çµ±ä¸­å¸¸ç”¨çš„ä¸‰å€‹ä¸»è¦å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•ï¼šQ-Learningã€SARSA å’Œ Deep Q-Network (DQN)ã€‚

---

## ğŸ§  **Q-Learning æ¼”ç®—æ³•**

### æ¼”ç®—æ³•æ¦‚è¿°

Q-Learning æ˜¯ä¸€ç¨®ç„¡æ¨¡å‹ (model-free) çš„é›¢ç­–ç•¥ (off-policy) æ™‚é–“å·®åˆ†å­¸ç¿’æ¼”ç®—æ³•ï¼Œç”¨æ–¼å­¸ç¿’æœ€å„ªå‹•ä½œ-åƒ¹å€¼å‡½æ•¸ã€‚

### æ ¸å¿ƒå…¬å¼

#### Qå€¼æ›´æ–°å…¬å¼

```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
                                   a'
```

**å…¬å¼è§£é‡‹ï¼š**

- `Q(s,a)`: åœ¨ç‹€æ…‹ s ä¸‹åŸ·è¡Œå‹•ä½œ a çš„ Q å€¼
- `Î±`: å­¸ç¿’ç‡ (learning rate)ï¼Œç¯„åœ [0,1]
- `r`: å³æ™‚çå‹µ (immediate reward)
- `Î³`: æŠ˜æ‰£å› å­ (discount factor)ï¼Œç¯„åœ [0,1]
- `s'`: ä¸‹ä¸€å€‹ç‹€æ…‹ (next state)
- `max Q(s',a')`: ä¸‹ä¸€ç‹€æ…‹æ‰€æœ‰å¯èƒ½å‹•ä½œçš„æœ€å¤§ Q å€¼
     a'

#### å‹•ä½œé¸æ“‡ç­–ç•¥ (Îµ-greedy)

```
Ï€(s) = {
  argmax Q(s,a)     with probability (1-Îµ)
     a
  random action     with probability Îµ
}
```

### åœ¨æ‚¨çš„ç³»çµ±ä¸­çš„å¯¦ç¾

æ ¹æ“š `agent.py` ä¸­çš„å¯¦ç¾ï¼š

```python
def update_q_table(self, state, action, reward, next_state):
    """Q-Learning æ›´æ–°è¦å‰‡"""
    best_next_action = np.argmax(self.q_table[next_state])
    self.q_table[state][action] = (1 - self.learning_rate) * self.q_table[state][action] + \
                                 self.learning_rate * (reward + self.discount_factor * 
                                                     self.q_table[next_state][best_next_action])
```

### ç‰¹é»

- âœ… **é›¢ç­–ç•¥å­¸ç¿’**ï¼šå¯ä»¥å¾ä»»ä½•ç­–ç•¥ç”Ÿæˆçš„æ•¸æ“šä¸­å­¸ç¿’
- âœ… **æ”¶æ–‚ä¿è­‰**ï¼šåœ¨æ»¿è¶³ä¸€å®šæ¢ä»¶ä¸‹ä¿è­‰æ”¶æ–‚åˆ°æœ€å„ªè§£
- âœ… **ç°¡å–®å¯¦ç¾**ï¼šä½¿ç”¨è¡¨æ ¼å½¢å¼å­˜å„² Q å€¼
- âš ï¸ **ç¶­åº¦é™åˆ¶**ï¼šç‹€æ…‹ç©ºé–“éå¤§æ™‚è¡¨æ ¼æœƒéæ–¼é¾å¤§

---

## ğŸ¯ **SARSA æ¼”ç®—æ³• (State-Action-Reward-State-Action)**

### æ¼”ç®—æ³•æ¦‚è¿°

SARSA æ˜¯ä¸€ç¨®åœ¨ç­–ç•¥ (on-policy) æ™‚é–“å·®åˆ†å­¸ç¿’æ¼”ç®—æ³•ï¼Œå­¸ç¿’ç•¶å‰éµå¾ªç­–ç•¥çš„å‹•ä½œ-åƒ¹å€¼å‡½æ•¸ã€‚

### æ ¸å¿ƒå…¬å¼

#### Qå€¼æ›´æ–°å…¬å¼

```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```

**å…¬å¼è§£é‡‹ï¼š**

- èˆ‡ Q-Learning çš„ä¸»è¦å·®åˆ¥ï¼šä½¿ç”¨å¯¦éš›åŸ·è¡Œçš„å‹•ä½œ `a'` è€Œéæœ€å„ªå‹•ä½œ
- `a'`: åœ¨ä¸‹ä¸€ç‹€æ…‹ s' å¯¦éš›é¸æ“‡çš„å‹•ä½œï¼ˆè€Œéæœ€å„ªå‹•ä½œï¼‰

#### å®Œæ•´æ¼”ç®—æ³•æµç¨‹

```
1. åˆå§‹åŒ– Q(s,a) ä»»æ„å€¼
2. é¸æ“‡åˆå§‹ç‹€æ…‹ sï¼Œä½¿ç”¨ç­–ç•¥é¸æ“‡å‹•ä½œ a
3. é‡è¤‡ä»¥ä¸‹æ­¥é©Ÿï¼š
   - åŸ·è¡Œå‹•ä½œ aï¼Œè§€å¯Ÿçå‹µ r å’Œæ–°ç‹€æ…‹ s'
   - åœ¨ s' ä½¿ç”¨ç­–ç•¥é¸æ“‡å‹•ä½œ a'
   - æ›´æ–° Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
   - s â† s', a â† a'
```

### èˆ‡ Q-Learning çš„æ¯”è¼ƒ

| ç‰¹æ€§ | Q-Learning | SARSA |
|------|------------|-------|
| ç­–ç•¥é¡å‹ | é›¢ç­–ç•¥ (Off-policy) | åœ¨ç­–ç•¥ (On-policy) |
| æ›´æ–°ç›®æ¨™ | max Q(s',a') | Q(s',a') |
| å­¸ç¿’å…§å®¹ | æœ€å„ªç­–ç•¥ | ç•¶å‰ç­–ç•¥ |
| æ”¶æ–‚æ€§ | æ”¶æ–‚åˆ°æœ€å„ªè§£ | æ”¶æ–‚åˆ°ç•¶å‰ç­–ç•¥çš„è§£ |
| æ¢ç´¢é¢¨éšª | è¼ƒç©æ¥µ | è¼ƒä¿å®ˆ |

### é©ç”¨å ´æ™¯

- ğŸš— **å®‰å…¨é—œéµæ‡‰ç”¨**ï¼šè»Šè¼›å°èˆªä¸­éœ€è¦ä¿å®ˆç­–ç•¥
- ğŸ® **åœ¨ç·šå­¸ç¿’**ï¼šéœ€è¦é‚Šå­¸ç¿’é‚ŠåŸ·è¡Œçš„ç’°å¢ƒ
- ğŸ›¡ï¸ **é¢¨éšªæ•æ„Ÿ**ï¼šä¸èƒ½æ‰¿å—æ¢ç´¢éç¨‹ä¸­çš„é«˜é¢¨éšª

---

## ğŸ”® **Deep Q-Network (DQN) æ¼”ç®—æ³•**

### æ¼”ç®—æ³•æ¦‚è¿°

DQN çµåˆæ·±åº¦ç¥ç¶“ç¶²çµ¡å’Œ Q-Learningï¼Œèƒ½å¤ è™•ç†é«˜ç¶­ç‹€æ…‹ç©ºé–“ï¼Œæ˜¯æ·±åº¦å¼·åŒ–å­¸ç¿’çš„çªç ´æ€§é€²å±•ã€‚

### æ ¸å¿ƒå…¬å¼

#### æå¤±å‡½æ•¸ (Loss Function)

```
L(Î¸) = E[(y - Q(s,a;Î¸))Â²]
```

å…¶ä¸­ç›®æ¨™å€¼ï¼š

```
y = r + Î³ max Q(s',a';Î¸â»)
              a'
```

**å…¬å¼è§£é‡‹ï¼š**

- `Î¸`: ç¥ç¶“ç¶²çµ¡åƒæ•¸
- `Î¸â»`: ç›®æ¨™ç¶²çµ¡åƒæ•¸ï¼ˆå®šæœŸå¾ä¸»ç¶²çµ¡è¤‡è£½ï¼‰
- `Q(s,a;Î¸)`: ä¸»ç¶²çµ¡çš„ Q å€¼é æ¸¬
- `Q(s',a';Î¸â»)`: ç›®æ¨™ç¶²çµ¡çš„ Q å€¼é æ¸¬

#### æ¢¯åº¦æ›´æ–°

```
Î¸ â† Î¸ - Î± âˆ‡Î¸ L(Î¸)
```

### é—œéµæŠ€è¡“

#### 1. ç¶“é©—å›æ”¾ (Experience Replay)

```
Buffer: (s, a, r, s') çš„è½‰æ›æ¨£æœ¬
éš¨æ©Ÿæ¡æ¨£æ‰¹æ¬¡é€²è¡Œè¨“ç·´ï¼Œæ‰“ç ´æ•¸æ“šç›¸é—œæ€§
```

#### 2. ç›®æ¨™ç¶²çµ¡ (Target Network)

```
Î¸â» â† Î¸    (æ¯ C æ­¥æ›´æ–°ä¸€æ¬¡)
```

#### 3. ç¶²çµ¡çµæ§‹ï¼ˆè»Šè¼›å°èˆªç³»çµ±é©ç”¨ï¼‰

```
è¼¸å…¥å±¤ï¼šç‹€æ…‹è¡¨ç¤º (ä½ç½®ã€æ“å¡åº¦ã€ç›®æ¨™ç­‰)
    â†“
éš±è—å±¤ï¼šå…¨é€£æ¥æˆ–å·ç©å±¤
    â†“
è¼¸å‡ºå±¤ï¼š4å€‹å‹•ä½œçš„ Q å€¼ (ä¸Šã€ä¸‹ã€å·¦ã€å³)
```

### DQN æ¼”ç®—æ³•å½ä»£ç¢¼

```python
def dqn_algorithm():
    # åˆå§‹åŒ–ç¶“é©—å›æ”¾ç·©è¡å€å’Œç¥ç¶“ç¶²çµ¡
    replay_buffer = ReplayBuffer()
    main_network = QNetwork()
    target_network = copy(main_network)
    
    for episode in range(num_episodes):
        state = env.reset()
        for step in range(max_steps):
            # Îµ-greedy å‹•ä½œé¸æ“‡
            action = select_action(state, main_network, epsilon)
            
            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, done = env.step(action)
            
            # å­˜å„²ç¶“é©—
            replay_buffer.store(state, action, reward, next_state, done)
            
            # å¾ç·©è¡å€æ¡æ¨£ä¸¦è¨“ç·´
            if len(replay_buffer) > batch_size:
                batch = replay_buffer.sample(batch_size)
                loss = compute_loss(batch, main_network, target_network)
                update_network(main_network, loss)
            
            # å®šæœŸæ›´æ–°ç›®æ¨™ç¶²çµ¡
            if step % target_update_freq == 0:
                target_network = copy(main_network)
                
            state = next_state
```

### åœ¨è»Šè¼›å°èˆªä¸­çš„æ‡‰ç”¨

#### ç‹€æ…‹ç©ºé–“è¨­è¨ˆ

```python
state = [
    vehicle_x,           # è»Šè¼› x åº§æ¨™
    vehicle_y,           # è»Šè¼› y åº§æ¨™
    destination_x,       # ç›®æ¨™ x åº§æ¨™
    destination_y,       # ç›®æ¨™ y åº§æ¨™
    congestion_map,      # å‘¨åœæ“å¡åº¦çŸ©é™£
    traffic_lights,      # äº¤é€šç‡ˆç‹€æ…‹
    distance_to_goal     # åˆ°ç›®æ¨™çš„è·é›¢
]
```

#### å‹•ä½œç©ºé–“

```python
actions = [
    0: å‘ä¸Šç§»å‹•
    1: å‘å³ç§»å‹•
    2: å‘ä¸‹ç§»å‹•
    3: å‘å·¦ç§»å‹•
]
```

#### çå‹µå‡½æ•¸ï¼ˆå¯çµåˆæ‚¨ç¾æœ‰çš„è¨­è¨ˆï¼‰

```python
def calculate_reward(state, action, next_state):
    reward = 0
    
    # åŸºæœ¬æ­¥æ•¸æ‡²ç½°
    reward += step_penalty
    
    # åˆ°é”ç›®æ¨™çå‹µ
    if reached_destination(next_state):
        reward += destination_reward
    
    # è·é›¢æ”¹å–„çå‹µ
    if distance_improved(state, next_state):
        reward += distance_reward
    
    # æ“å¡æ‡²ç½°
    if in_congested_area(next_state):
        reward += congestion_penalty
    
    return reward
```

---

## ğŸ“Š **ä¸‰ç¨®æ¼”ç®—æ³•æ¯”è¼ƒåˆ†æ**

### æ€§èƒ½å°æ¯”è¡¨

| è©•ä¼°æŒ‡æ¨™ | Q-Learning | SARSA | DQN |
|---------|------------|-------|-----|
| **ç‹€æ…‹ç©ºé–“é©ç”¨æ€§** | å°å‹é›¢æ•£ | å°å‹é›¢æ•£ | å¤§å‹é€£çºŒ/é›¢æ•£ |
| **å­¸ç¿’é€Ÿåº¦** | ä¸­ç­‰ | ä¸­ç­‰ | è¼ƒæ…¢ï¼ˆéœ€è¦å¤§é‡æ•¸æ“šï¼‰ |
| **å…§å­˜éœ€æ±‚** | ä½ | ä½ | é«˜ |
| **å¯¦ç¾è¤‡é›œåº¦** | ç°¡å–® | ç°¡å–® | è¤‡é›œ |
| **æ”¶æ–‚ç©©å®šæ€§** | å¥½ | å¥½ | éœ€è¦æŠ€å·§ |
| **æ³›åŒ–èƒ½åŠ›** | å·® | å·® | å¼· |

### é©ç”¨å ´æ™¯å»ºè­°

#### ğŸ¯ ä½¿ç”¨ Q-Learning ç•¶

- ç‹€æ…‹ç©ºé–“è¼ƒå°ï¼ˆå¦‚20x20ç¶²æ ¼ï¼‰
- éœ€è¦å¿«é€ŸåŸå‹é–‹ç™¼
- è¦æ±‚æ¼”ç®—æ³•é€æ˜åº¦é«˜
- è¨ˆç®—è³‡æºæœ‰é™

#### ğŸ›¡ï¸ ä½¿ç”¨ SARSA ç•¶

- å®‰å…¨æ€§è¦æ±‚é«˜ï¼ˆé¿å…å±éšªæ¢ç´¢ï¼‰
- åœ¨ç·šå­¸ç¿’ç’°å¢ƒ
- éœ€è¦ä¿å®ˆçš„å­¸ç¿’ç­–ç•¥
- ç’°å¢ƒä¸­å­˜åœ¨æ‡²ç½°æ€§é™·é˜±

#### ğŸ”® ä½¿ç”¨ DQN ç•¶

- ç‹€æ…‹ç©ºé–“å¾ˆå¤§æˆ–é€£çºŒ
- æœ‰å……è¶³çš„è¨ˆç®—è³‡æº
- éœ€è¦è™•ç†åŸå§‹æ„ŸçŸ¥è¼¸å…¥ï¼ˆåœ–åƒç­‰ï¼‰
- è¦æ±‚é«˜æ³›åŒ–èƒ½åŠ›

---

## ğŸ’¡ **åœ¨æ‚¨çš„è»Šè¼›å°èˆªç³»çµ±ä¸­çš„æ‡‰ç”¨å»ºè­°**

### ç•¶å‰ç³»çµ±åˆ†æ

æ‚¨çš„ç³»çµ±ç›®å‰å¯¦ç¾äº† **Q-Learning** æ¼”ç®—æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š

- 20x20 ç¶²æ ¼ç’°å¢ƒ âœ… é©åˆ Q-Learning
- é›¢æ•£ç‹€æ…‹å’Œå‹•ä½œç©ºé–“ âœ…
- è¡¨æ ¼å‹ Q å€¼å­˜å„² âœ…
- Îµ-greedy æ¢ç´¢ç­–ç•¥ âœ…

### æ“´å±•å»ºè­°

#### 1. æ·»åŠ  SARSA æ”¯æŒ

```python
class SARSAAgent(QLearningAgent):
    def update_q_table(self, state, action, reward, next_state, next_action):
        """SARSA æ›´æ–°è¦å‰‡ - ä½¿ç”¨å¯¦éš›é¸æ“‡çš„ä¸‹ä¸€å‹•ä½œ"""
        self.q_table[state][action] = (1 - self.learning_rate) * self.q_table[state][action] + \
                                     self.learning_rate * (reward + self.discount_factor * 
                                                         self.q_table[next_state][next_action])
```

#### 2. æ·»åŠ  DQN æ”¯æŒï¼ˆé€²éšï¼‰

```python
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()
        self.target_model = self._build_model()
        
    def _build_model(self):
        # æ§‹å»ºç¥ç¶“ç¶²çµ¡
        model = Sequential()
        model.add(Dense(64, input_dim=self.state_size, activation='relu'))
        model.add(Dense(64, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))
        return model
```

### é¸æ“‡å»ºè­°

åŸºæ–¼æ‚¨ç¾æœ‰çš„ç³»çµ±è¦æ¨¡å’Œè¤‡é›œåº¦ï¼Œå»ºè­°ï¼š

1. **ä¿æŒ Q-Learning ä½œç‚ºä¸»è¦æ¼”ç®—æ³•** - é©åˆç•¶å‰ 20x20 ç¶²æ ¼
2. **è€ƒæ…®æ·»åŠ  SARSA** - æä¾›æ›´å®‰å…¨çš„å­¸ç¿’é¸é …
3. **DQN ä½œç‚ºæœªä¾†æ“´å±•** - ç•¶éœ€è¦è™•ç†æ›´å¤§ç‹€æ…‹ç©ºé–“æ™‚

é€™æ¨£æ‚¨å°±æœ‰äº†å®Œæ•´çš„å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•å·¥å…·ç®±ï¼Œå¯ä»¥æ ¹æ“šä¸åŒéœ€æ±‚é¸æ“‡æœ€é©åˆçš„æ–¹æ³•ï¼

---

## ğŸ”§ **å¯¦éš›å¯¦ç¾åƒè€ƒä»£ç¢¼**

### Q-Learning æ ¸å¿ƒå¾ªç’°

```python
def q_learning_episode(agent, env):
    state = env.reset()
    total_reward = 0
    
    while not done:
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)
        
        # Q-Learning æ›´æ–°ï¼ˆä½¿ç”¨æœ€å„ªå‹•ä½œï¼‰
        agent.update_q_table(state, action, reward, next_state)
        
        state = next_state
        total_reward += reward
    
    return total_reward
```

### SARSA æ ¸å¿ƒå¾ªç’°

```python
def sarsa_episode(agent, env):
    state = env.reset()
    action = agent.choose_action(state)
    total_reward = 0
    
    while not done:
        next_state, reward, done = env.step(action)
        next_action = agent.choose_action(next_state) if not done else None
        
        # SARSA æ›´æ–°ï¼ˆä½¿ç”¨å¯¦éš›é¸æ“‡çš„å‹•ä½œï¼‰
        agent.update_q_table(state, action, reward, next_state, next_action)
        
        state = next_state
        action = next_action
        total_reward += reward
    
    return total_reward
```

é€™ä»½æ–‡æª”æä¾›äº†ä¸‰ç¨®æ¼”ç®—æ³•çš„å®Œæ•´å…¬å¼å’Œå¯¦ç¾æŒ‡å°ï¼Œå¯ä»¥å¹«åŠ©æ‚¨ç†è§£å’Œæ“´å±•ç¾æœ‰çš„è»Šè¼›å°èˆªç³»çµ±ï¼
