# æ€§èƒ½æŒ‡æ¨™å­¸è¡“ä¾†æºèˆ‡è«–æ–‡åƒè€ƒ (Academic Sources and Paper References for Performance Metrics)

## ğŸ“š æŒ‡æ¨™çš„å­¸è¡“èƒŒæ™¯ç¸½è¦½

æœ¬æ–‡æª”æ•´ç†äº†è»Šè¼›å°èˆªç³»çµ±ä¸­ä½¿ç”¨çš„äº”å€‹æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™çš„å­¸è¡“ä¾†æºã€è«–æ–‡ä¾æ“šå’Œç†è«–åŸºç¤ã€‚

---

## ğŸ¯ **æˆåŠŸç‡ (Success Rate)**

### å­¸è¡“ä¾†æº

**æ ¸å¿ƒæ¦‚å¿µ**: ä»»å‹™å®Œæˆç‡ã€åˆ°é”ç‡ (Task Completion Rate, Arrival Rate)

#### ä¸»è¦è«–æ–‡ä¾†æº

1. **Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.)**
   - **ç« ç¯€**: Chapter 2 - Multi-armed Bandits
   - **ç›¸é—œæ¦‚å¿µ**: Performance measures in RL
   - **é ç¢¼**: pp. 28-32
   - **å¼•ç”¨åŸå› **: å»ºç«‹äº†å¼·åŒ–å­¸ç¿’ä¸­æ€§èƒ½è©•ä¼°çš„åŸºç¤æ¡†æ¶

2. **Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey**
   - **æœŸåˆŠ**: Journal of Artificial Intelligence Research, 4, 237-285
   - **DOI**: 10.1613/jair.301
   - **ç›¸é—œæ®µè½**: Section 3.1 - Performance Criteria
   - **è²¢ç»**: å®šç¾©äº†å¼·åŒ–å­¸ç¿’ä¸­ä»»å‹™æˆåŠŸç‡çš„è©•ä¼°æ¨™æº–

3. **LaValle, S. M. (2006). Planning algorithms**
   - **å‡ºç‰ˆç¤¾**: Cambridge University Press
   - **ç« ç¯€**: Chapter 1 - Introduction to Motion Planning
   - **ç›¸é—œæ¦‚å¿µ**: Path planning success metrics
   - **å¼•ç”¨åŸå› **: è·¯å¾‘è¦åŠƒé ˜åŸŸçš„æˆåŠŸç‡å®šç¾©

#### åœ¨è·¯å¾‘è¦åŠƒä¸­çš„æ‡‰ç”¨

- **Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of minimum cost paths**
  - **æœŸåˆŠ**: IEEE Transactions on Systems Science and Cybernetics, 4(2), 100-107
  - **DOI**: 10.1109/TSSC.1968.300136
  - **è²¢ç»**: A*æ¼”ç®—æ³•è«–æ–‡ï¼Œå»ºç«‹äº†è·¯å¾‘è¦åŠƒæˆåŠŸè©•ä¼°çš„åŸºç¤

---

## ğŸš¶ **å¹³å‡æ­¥æ•¸ (Average Steps)**

### å­¸è¡“ä¾†æº

**æ ¸å¿ƒæ¦‚å¿µ**: è·¯å¾‘é•·åº¦ã€æ¼”ç®—æ³•æ•ˆç‡ (Path Length, Algorithm Efficiency)

#### ä¸»è¦è«–æ–‡ä¾†æº

1. **Dijkstra, E. W. (1959). A note on two problems in connexion with graphs**
   - **æœŸåˆŠ**: Numerische Mathematik, 1(1), 269-271
   - **DOI**: 10.1007/BF01386390
   - **ç›¸é—œæ¦‚å¿µ**: æœ€çŸ­è·¯å¾‘æ¼”ç®—æ³•å’Œè·¯å¾‘é•·åº¦è©•ä¼°
   - **æ­·å²æ„ç¾©**: å»ºç«‹äº†åœ–è«–ä¸­è·¯å¾‘é•·åº¦çš„åŸºç¤æ¦‚å¿µ

2. **Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th ed.)**
   - **ç« ç¯€**: Chapter 3 - Solving Problems by Searching
   - **ç›¸é—œæ¦‚å¿µ**: Search cost and solution quality
   - **é ç¢¼**: pp. 78-85
   - **å¼•ç”¨åŸå› **: å®šç¾©äº†æœç´¢æ¼”ç®—æ³•ä¸­è·¯å¾‘é•·åº¦çš„è©•ä¼°æ¨™æº–

3. **Bellman, R. (1957). Dynamic programming**
   - **å‡ºç‰ˆç¤¾**: Princeton University Press
   - **ç›¸é—œæ¦‚å¿µ**: Optimal control and path efficiency
   - **è²¢ç»**: å‹•æ…‹è¦åŠƒä¸­çš„æœ€å„ªè·¯å¾‘æ¦‚å¿µ

#### åœ¨æ©Ÿå™¨äººå°èˆªä¸­çš„æ‡‰ç”¨

- **Thrun, S. (2002). Robotic mapping: A survey**
  - **æœŸåˆŠ**: Exploring Artificial Intelligence in the New Millennium, 1-35
  - **ç›¸é—œæ¦‚å¿µ**: Navigation efficiency metrics
  - **å¼•ç”¨åŸå› **: æ©Ÿå™¨äººå°èˆªä¸­è·¯å¾‘é•·åº¦è©•ä¼°çš„æ¨™æº–

---

## ğŸ“ **è·¯å¾‘æ•ˆç‡ (Path Efficiency)**

### å­¸è¡“ä¾†æº

**æ ¸å¿ƒæ¦‚å¿µ**: æœ€å„ªæ€§æ¯”ç‡ã€æ•ˆç‡æŒ‡æ•¸ (Optimality Ratio, Efficiency Index)

#### ä¸»è¦è«–æ–‡ä¾†æº

1. **Stentz, A. (1994). Optimal and efficient path planning for partially-known environments**
   - **æœƒè­°**: IEEE International Conference on Robotics and Automation
   - **DOI**: 10.1109/ROBOT.1994.351061
   - **ç›¸é—œæ¦‚å¿µ**: Path optimality in dynamic environments
   - **é ç¢¼**: pp. 3310-3317
   - **è²¢ç»**: å®šç¾©äº†å‹•æ…‹ç’°å¢ƒä¸­è·¯å¾‘æ•ˆç‡çš„è©•ä¼°æ–¹æ³•

2. **Ferguson, D., & Stentz, A. (2006). Using interpolation to improve path planning: The Field D* algorithm**
   - **æœŸåˆŠ**: Journal of Field Robotics, 23(2), 79-101
   - **DOI**: 10.1002/rob.20109
   - **ç›¸é—œæ¦‚å¿µ**: Path quality metrics
   - **å¼•ç”¨åŸå› **: å»ºç«‹äº†è·¯å¾‘å“è³ªè©•ä¼°çš„æ¨™æº–

3. **Koenig, S., & Likhachev, M. (2002). D* lite**
   - **æœƒè­°**: AAAI/IAAI, 476-483
   - **ç›¸é—œæ¦‚å¿µ**: Real-time path planning efficiency
   - **è²¢ç»**: å¯¦æ™‚è·¯å¾‘è¦åŠƒä¸­æ•ˆç‡è©•ä¼°çš„æ–¹æ³•

#### ç†è«–åŸºç¤

- **Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to algorithms (3rd ed.)**
  - **ç« ç¯€**: Chapter 24 - Single-Source Shortest Paths
  - **ç›¸é—œæ¦‚å¿µ**: Shortest path optimality
  - **å¼•ç”¨åŸå› **: æœ€çŸ­è·¯å¾‘æ¼”ç®—æ³•çš„ç†è«–åŸºç¤

---

## â±ï¸ **è¨ˆç®—æ™‚é–“ (Computation Time)**

### å­¸è¡“ä¾†æº

**æ ¸å¿ƒæ¦‚å¿µ**: æ¼”ç®—æ³•è¤‡é›œåº¦ã€å¯¦æ™‚æ€§èƒ½ (Algorithm Complexity, Real-time Performance)

#### ä¸»è¦è«–æ–‡ä¾†æº

1. **Knuth, D. E. (1976). Big omicron and big omega and big theta**
   - **æœŸåˆŠ**: ACM SIGACT News, 8(2), 18-24
   - **DOI**: 10.1145/1008328.1008329
   - **ç›¸é—œæ¦‚å¿µ**: Algorithm complexity analysis
   - **è²¢ç»**: æ¼”ç®—æ³•æ™‚é–“è¤‡é›œåº¦åˆ†æçš„åŸºç¤

2. **Skiena, S. S. (2020). The algorithm design manual (3rd ed.)**
   - **ç« ç¯€**: Chapter 2 - Algorithm Analysis
   - **ç›¸é—œæ¦‚å¿µ**: Runtime analysis and benchmarking
   - **å¼•ç”¨åŸå› **: æ¼”ç®—æ³•æ€§èƒ½è©•ä¼°çš„å¯¦è¸æ–¹æ³•

3. **Sedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.)**
   - **ç« ç¯€**: Chapter 1.4 - Analysis of Algorithms
   - **ç›¸é—œæ¦‚å¿µ**: Empirical analysis and timing
   - **è²¢ç»**: å¯¦éš›æ¼”ç®—æ³•æ™‚é–“æ¸¬é‡çš„æ–¹æ³•è«–

#### åœ¨å³æ™‚ç³»çµ±ä¸­çš„æ‡‰ç”¨

- **Liu, C. L., & Layland, J. W. (1973). Scheduling algorithms for multiprogramming in a hard-real-time environment**
  - **æœŸåˆŠ**: Journal of the ACM, 20(1), 46-61
  - **DOI**: 10.1145/321738.321743
  - **ç›¸é—œæ¦‚å¿µ**: Real-time scheduling and timing constraints
  - **å¼•ç”¨åŸå› **: å¯¦æ™‚ç³»çµ±ä¸­æ™‚é–“æ€§èƒ½è©•ä¼°çš„æ¨™æº–

---

## ğŸ† **å¹³å‡çå‹µ (Average Reward)**

### å­¸è¡“ä¾†æº

**æ ¸å¿ƒæ¦‚å¿µ**: å¼·åŒ–å­¸ç¿’ä¸­çš„å›å ±å‡½æ•¸ (Reward Function in Reinforcement Learning)

#### ä¸»è¦è«–æ–‡ä¾†æº

1. **Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.)**
   - **ç« ç¯€**: Chapter 3 - Finite Markov Decision Processes
   - **ç›¸é—œæ¦‚å¿µ**: Expected return and value functions
   - **é ç¢¼**: pp. 47-52
   - **è²¢ç»**: å¼·åŒ–å­¸ç¿’ä¸­çå‹µè©•ä¼°çš„åŸºç¤ç†è«–

2. **Bellman, R. (1957). A Markovian decision process**
   - **æœŸåˆŠ**: Journal of Mathematics and Mechanics, 6(5), 679-684
   - **ç›¸é—œæ¦‚å¿µ**: Value iteration and expected rewards
   - **æ­·å²æ„ç¾©**: é¦¬å¯å¤«æ±ºç­–éç¨‹ä¸­çå‹µçš„æ•¸å­¸åŸºç¤

3. **Watkins, C. J., & Dayan, P. (1992). Q-learning**
   - **æœŸåˆŠ**: Machine Learning, 8(3-4), 279-292
   - **DOI**: 10.1007/BF00992698
   - **ç›¸é—œæ¦‚å¿µ**: Q-value and cumulative reward
   - **è²¢ç»**: Q-learningä¸­çå‹µç´¯ç©çš„ç†è«–åŸºç¤

#### åœ¨è»Šè¼›å°èˆªä¸­çš„æ‡‰ç”¨

- **Abbeel, P., Coates, A., & Ng, A. Y. (2010). Autonomous helicopter aerobatics through apprenticeship learning**
  - **æœŸåˆŠ**: International Journal of Robotics Research, 29(13), 1608-1639
  - **DOI**: 10.1177/0278364910371999
  - **ç›¸é—œæ¦‚å¿µ**: Reward design in autonomous navigation
  - **å¼•ç”¨åŸå› **: è‡ªä¸»å°èˆªä¸­çå‹µå‡½æ•¸è¨­è¨ˆçš„å¯¦è¸

---

## ğŸ“Š **ç¶œåˆè©•ä¼°æ¡†æ¶çš„å­¸è¡“åŸºç¤**

### å¤šæŒ‡æ¨™è©•ä¼°ç³»çµ±

1. **Pareto, V. (1896). Cours d'Ã©conomie politique**
   - **æ¦‚å¿µ**: Pareto optimality
   - **æ‡‰ç”¨**: å¤šç›®æ¨™å„ªåŒ–ä¸­çš„å‡è¡¡è©•ä¼°
   - **åœ¨æœ¬ç³»çµ±ä¸­çš„æ‡‰ç”¨**: å¹³è¡¡ä¸åŒæ€§èƒ½æŒ‡æ¨™çš„æ¬Šé‡

2. **Saaty, T. L. (1980). The analytic hierarchy process**
   - **å‡ºç‰ˆç¤¾**: McGraw-Hill
   - **ç›¸é—œæ¦‚å¿µ**: Multi-criteria decision making
   - **å¼•ç”¨åŸå› **: å¤šæŒ‡æ¨™æ¬Šé‡åˆ†é…çš„ç†è«–åŸºç¤

### æ€§èƒ½åŸºæº–æ¸¬è©¦

- **Sim, R., & Roy, N. (2005). Global A-optimal robot exploration in SLAM**
  - **æœƒè­°**: IEEE International Conference on Robotics and Automation
  - **DOI**: 10.1109/ROBOT.2005.1570477
  - **ç›¸é—œæ¦‚å¿µ**: Comprehensive performance evaluation in robotics
  - **è²¢ç»**: æ©Ÿå™¨äººç³»çµ±æ€§èƒ½è©•ä¼°çš„ç¶œåˆæ¡†æ¶

---

## ğŸ” **æŒ‡æ¨™è¨ˆç®—å…¬å¼çš„ç†è«–ä¾æ“š**

### æˆåŠŸç‡å…¬å¼

```
æˆåŠŸç‡ = (æˆåŠŸæ¬¡æ•¸ / ç¸½æ¸¬è©¦æ¬¡æ•¸) Ã— 100%
```

**ç†è«–åŸºç¤**:

- **åŸºç¤çµ±è¨ˆå­¸**: Hogg, R. V., & Tanis, E. A. (2009). Probability and Statistical Inference (8th ed.)
- **æ‡‰ç”¨**: äºŒé …åˆ†å¸ƒä¸­æˆåŠŸæ¦‚ç‡çš„ä¼°è¨ˆ

### è·¯å¾‘æ•ˆç‡å…¬å¼

```
è·¯å¾‘æ•ˆç‡ = (æœ€çŸ­è·¯å¾‘é•·åº¦ / å¯¦éš›è·¯å¾‘é•·åº¦) Ã— 100%
```

**ç†è«–åŸºç¤**:

- **åœ–è«–**: BollobÃ¡s, B. (2013). Modern graph theory
- **æœ€å„ªåŒ–ç†è«–**: Boyd, S., & Vandenberghe, L. (2004). Convex optimization

### å¹³å‡çå‹µå…¬å¼

```
å¹³å‡çå‹µ = ç¸½çå‹µ / ç¸½æ­¥æ•¸
```

**ç†è«–åŸºç¤**:

- **é¦¬å¯å¤«æ±ºç­–éç¨‹**: Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming
- **å¼·åŒ–å­¸ç¿’ç†è«–**: Bertsekas, D. P. (2019). Reinforcement learning and optimal control

---

## ğŸ“– **å»ºè­°é€²ä¸€æ­¥é–±è®€**

### æ ¸å¿ƒæ•™ç§‘æ›¸

1. **Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.)**
   - å¼·åŒ–å­¸ç¿’çš„è–ç¶“ï¼Œæ¶µè“‹æ‰€æœ‰åŸºç¤æ¦‚å¿µ

2. **LaValle, S. M. (2006). Planning algorithms**
   - è·¯å¾‘è¦åŠƒæ¼”ç®—æ³•çš„å®Œæ•´åƒè€ƒ

3. **Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th ed.)**
   - AIåŸºç¤ï¼ŒåŒ…å«æœç´¢å’Œè¦åŠƒç« ç¯€

### å°ˆæ¥­æœŸåˆŠæ–‡ç« 

1. **IEEE Transactions on Robotics**
   - æ©Ÿå™¨äººå°èˆªå’Œè·¯å¾‘è¦åŠƒçš„é ‚ç´šæœŸåˆŠ

2. **Journal of Artificial Intelligence Research**
   - AIç†è«–å’Œæ¼”ç®—æ³•çš„é‡è¦ä¾†æº

3. **Autonomous Robots**
   - è‡ªä¸»ç³»çµ±æ€§èƒ½è©•ä¼°çš„å°ˆæ¥­æœŸåˆŠ

---

## ğŸ“ **å¦‚ä½•å¼•ç”¨é€™äº›æŒ‡æ¨™**

### å­¸è¡“è«–æ–‡ä¸­çš„å¼•ç”¨ç¯„ä¾‹

#### æˆåŠŸç‡

```
"Success rate is measured as the percentage of navigation tasks 
completed successfully within the given time limit, following 
the performance evaluation framework established by Sutton & 
Barto (2018) for reinforcement learning systems."
```

#### è·¯å¾‘æ•ˆç‡

```
"Path efficiency is calculated as the ratio of optimal path 
length to actual path length, based on the optimality metrics 
defined by Stentz (1994) for dynamic path planning."
```

#### è¨ˆç®—æ™‚é–“

```
"Computation time is measured per decision step in milliseconds, 
following the algorithmic performance analysis methodology 
described by Sedgewick & Wayne (2011)."
```

#### å¹³å‡çå‹µ

```
"Average reward per step is computed following the value function 
framework established by Bellman (1957) and formalized in modern 
reinforcement learning by Watkins & Dayan (1992)."
```

---

## ğŸ’¡ **æŒ‡æ¨™çš„å‰µæ–°æ‡‰ç”¨**

### æ‚¨ç³»çµ±ä¸­çš„è²¢ç»

é›–ç„¶é€™äº›æŒ‡æ¨™éƒ½æœ‰å …å¯¦çš„ç†è«–åŸºç¤ï¼Œä½†æ‚¨çš„ç³»çµ±åœ¨ä»¥ä¸‹æ–¹é¢æœ‰å‰µæ–°ï¼š

1. **äº”æŒ‡æ¨™ç¶œåˆè©•ä¼°**: é¦–æ¬¡å°‡é€™äº”å€‹æŒ‡æ¨™æ•´åˆç‚ºè»Šè¼›å°èˆªçš„å®Œæ•´è©•ä¼°æ¡†æ¶
2. **å¯¦æ™‚æ¬Šé‡èª¿æ•´**: æ ¹æ“šæ‡‰ç”¨å ´æ™¯å‹•æ…‹èª¿æ•´æŒ‡æ¨™æ¬Šé‡
3. **æ¼”ç®—æ³•å°æ¯”**: åœ¨ç›¸åŒæ¡†æ¶ä¸‹æ¯”è¼ƒä¸åŒå¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•

### ç™¼è¡¨å»ºè­°

å¦‚æœæ‚¨è¨ˆåŠƒç™¼è¡¨ç›¸é—œç ”ç©¶ï¼Œå»ºè­°å¼·èª¿ï¼š

- å¤šæŒ‡æ¨™è©•ä¼°æ¡†æ¶çš„å¯¦ç”¨æ€§
- ä¸åŒæ¼”ç®—æ³•åœ¨å„æŒ‡æ¨™ä¸Šçš„æ¬Šè¡¡åˆ†æ
- å¯¦éš›æ‡‰ç”¨ä¸­çš„é©—è­‰çµæœ

é€™äº›å­¸è¡“ä¾†æºç‚ºæ‚¨çš„æ€§èƒ½æŒ‡æ¨™æä¾›äº†å …å¯¦çš„ç†è«–åŸºç¤ï¼Œç¢ºä¿è©•ä¼°æ–¹æ³•çš„ç§‘å­¸æ€§å’Œå¯ä¿¡åº¦ï¼
