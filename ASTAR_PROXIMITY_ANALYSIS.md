# A*æ¼”ç®—æ³•+é„°è¿‘æ¼”ç®—æ³•+Q-Learning èˆ‡ ç´”æŒ‡æ•¸è·é›¢æ¼”ç®—æ³• çš„æ¯”è¼ƒ

## æœ¬ç³»çµ±

### 1. é„°è¿‘æ€§æ¦‚å¿µåœ¨ç³»çµ±ä¸­çš„é«”ç¾

#### A. **å‹•ä½œé¸æ“‡çš„é„°è¿‘æ€§**

```python
def get_valid_actions(self, position):
    """Get valid actions at current position (avoiding grid boundaries and obstacles)"""
    valid_actions = []
    for i, (dx, dy) in enumerate(self.actions):  # [(0,1), (1,0), (0,-1), (-1,0)]
        new_x = position[0] + dx
        new_y = position[1] + dy
        # åªè€ƒæ…®ç›¸é„°çš„å››å€‹ä½ç½® - é€™å°±æ˜¯é„°è¿‘æ€§æ¦‚å¿µï¼
```

**é„°è¿‘æ€§é«”ç¾**ï¼š

- æ‚¨çš„ç³»çµ±åªè€ƒæ…®ç•¶å‰ä½ç½®çš„**ç›´æ¥ç›¸é„°ä½ç½®**ï¼ˆä¸Šä¸‹å·¦å³å››å€‹æ–¹å‘ï¼‰
- é€™æ­£æ˜¯é„°è¿‘æ¼”ç®—æ³•çš„æ ¸å¿ƒï¼š**å±€éƒ¨æœç´¢**ï¼Œåªè€ƒæ…®é™„è¿‘çš„é¸é …

#### B. **åŸºæ–¼è·é›¢çš„çå‹µæ©Ÿåˆ¶**

**ç¨‹å¼ç¢¼å¯¦ç¾**ï¼š

```python
def calculate_reward_proximity_based(self, new_position, dx, dy):
    # --- Proximity reward: the closer to the destination, the higher the reward ---
    old_dist = manhattan_dist(self.position, self.destination)
    new_dist = manhattan_dist(new_position, self.destination)
    proximity_reward = old_dist - new_dist  # è¶Šè¿‘çå‹µè¶Šé«˜
    
    # è·é›¢è¶Šè¿‘ï¼Œçå‹µå€æ•¸è¶Šé«˜
    progress = 1 - (new_dist / max_possible_dist)
    proximity_multiplier = base_multiplier + (max_multiplier * progress)
    reward += proximity_reward * proximity_multiplier
```

**æ•¸å­¸å…¬å¼ç‰ˆæœ¬**ï¼š

##### ğŸ“ åŸºç¤é„°è¿‘çå‹µå‡½æ•¸

**æ›¼å“ˆé “è·é›¢**ï¼š

$d(pâ‚, pâ‚‚) = |xâ‚ - xâ‚‚| + |yâ‚ - yâ‚‚|$

**é„°è¿‘æ€§çå‹µ**ï¼š

$R_{proximity} = d(s_t, g) - d(s_{t+1}, g)$

å…¶ä¸­ï¼š

- $s_t$ = ç•¶å‰ä½ç½® (current position)
- $s_{t+1}$ = ä¸‹ä¸€å€‹ä½ç½® (next position)  
- $g$ = ç›®æ¨™ä½ç½® (goal position)
- $d(\#,\#)$ = æ›¼å“ˆé “è·é›¢å‡½æ•¸

##### ğŸ“Š å‹•æ…‹å€æ•¸çå‹µå‡½æ•¸

**é€²åº¦è¨ˆç®—**ï¼š

$progress = 1 - d(s_{t+1}, g) / d_{max}$

**é„°è¿‘å€æ•¸**ï¼š

$M_{proximity} = Î±_{base} + Î±_{max} Ã— progress$

**æœ€çµ‚é„°è¿‘çå‹µ**ï¼š

$R_{proximity  final} = R_{proximity} Ã— M_{proximity}$

##### ğŸ¯ å®Œæ•´çå‹µå‡½æ•¸

**ç¸½çå‹µå‡½æ•¸**ï¼š

$R_{total} = R_{step} + R_{proximity final} + R_{astar} + R_{congestion} + R_{loop}$

å±•é–‹ç‚ºï¼š

$R_{total} = Î²_{step} + [d(s_t,g) - d(s_{t+1},g)] Ã— [Î±_{base} + Î±_{max} Ã— (1 - d(s_{t+1},g)/d_{max})] + R_{astar} + R_{congestion} + R_{loop}$

**åƒæ•¸èªªæ˜**ï¼š

- $Î²_{step} = åŸºç¤æ­¥æ•¸æ‡²ç½° (é€šå¸¸ç‚ºè² å€¼)$
- $Î±_{base} = åŸºç¤é„°è¿‘å€æ•¸$
- $Î±_{max} = æœ€å¤§é„°è¿‘å€æ•¸$
- $d_{max} = ç¶²æ ¼æœ€å¤§å¯èƒ½è·é›¢ = 2 Ã— grid size$
- $R_{asta} = \text{A*è·¯å¾‘è·Ÿéš¨çå‹µ}$
- $R_{congestion} = æ“å¡æ‡²ç½°$
- $R_{loop} = è¿´è·¯æ‡²ç½°$

##### ğŸ”¢ å…¸å‹åƒæ•¸å€¼

- $Î²_{step} = -1          \quad\text {æ¯æ­¥åŸºç¤æ‡²ç½°}$
- $Î±_{base} = 1.0         \quad\text { åŸºç¤å€æ•¸}$
- $Î±_{max} = 3.0          \quad\text { æœ€å¤§å€æ•¸}$
- $d_{max} = 40           \quad\text { å°æ–¼20Ã—20ç¶²æ ¼}$

##### ğŸ“ˆ çå‹µå‡½æ•¸æ€§è³ªåˆ†æ

**è·é›¢çå‹µç‰¹æ€§**ï¼š

- ç•¶ $d(s_{t+1}, g) < d(s_t, g)$ æ™‚ï¼Œ$R_{proximity} > 0$ (é è¿‘ç›®æ¨™)
- ç•¶ $d(s_{t+1}, g) > d(s_t, g)$ æ™‚ï¼Œ$R_{proximity} < 0$ (é é›¢ç›®æ¨™)
- ç•¶ $d(s_{t+1}, g) = d(s_t, g)$ æ™‚ï¼Œ$R_{proximity} = 0$ (è·é›¢ä¸è®Š)

**å€æ•¸æ•ˆæ‡‰**ï¼š

- è·é›¢ç›®æ¨™è¶Šè¿‘ï¼Œå€æ•¸è¶Šå¤§ï¼Œçå‹µæ”¾å¤§æ•ˆæœè¶Šå¼·
- åœ¨ç›®æ¨™é™„è¿‘ï¼š$M_{proximity} â‰ˆ Î±_{base} + Î±_{max} â‰ˆ 4.0$
- åœ¨èµ·å§‹ä½ç½®ï¼š$M_{proximity} â‰ˆ Î±_{base} â‰ˆ 1.0$

#### C. **ç‹€æ…‹è¡¨ç¤ºä¸­çš„æ–¹å‘æ„ŸçŸ¥**

```python
def get_state_key(self, position, congestion_level):
    # Get direction to destination (discretized to 8 directions)
    if hasattr(self, 'current_destination'):
        dx = self.current_destination[0] - position[0]
        dy = self.current_destination[1] - position[1]
        angle = np.arctan2(dy, dx)
        direction = int(((angle + np.pi) * 4 / np.pi + 0.5) % 8)
```

**é„°è¿‘æ€§é«”ç¾**ï¼š

- ç‹€æ…‹è¡¨ç¤ºåŒ…å«äº†**æœå‘ç›®æ¨™çš„æ–¹å‘**
- é€™å¹«åŠ©æ™ºèƒ½é«”å„ªå…ˆé¸æ“‡æœå‘ç›®æ¨™çš„ç›¸é„°ä½ç½®

### 2. æˆ‘å€‘çš„ç³»çµ± vs ç´”é„°è¿‘æ¼”ç®—æ³•

| ç‰¹æ€§ | ç´”é„°è¿‘æ¼”ç®—æ³• | A*+Q-learning+é„°è¿‘æ¼”ç®—æ³• |
|------|-------------|------------------|
| **æœç´¢ç¯„åœ** | åªè€ƒæ…®ç›¸é„°ä½ç½® | âœ… ä¹Ÿåªè€ƒæ…®ç›¸é„°ä½ç½® |
| **è·é›¢åå¥½** | é¸æ“‡è·é›¢ç›®æ¨™æœ€è¿‘çš„ç›¸é„°ä½ç½® | âœ… çå‹µé è¿‘ç›®æ¨™çš„ç§»å‹• |
| **æ±ºç­–æ©Ÿåˆ¶** | è²ªå©ªé¸æ“‡ | ğŸ”„ çµåˆå­¸ç¿’çš„è²ªå©ªé¸æ“‡ |
| **å…¨å±€è¦åŠƒ** | âŒ ç„¡ | âœ… æœ‰ï¼ˆA*æŒ‡å°ï¼‰ |
| **å­¸ç¿’èƒ½åŠ›** | âŒ ç„¡ | âœ… æœ‰ï¼ˆQ-learningï¼‰ |

### 3. å‰µæ–°ä¹‹è™•

#### A. **å¤šå±¤æ¬¡çš„é„°è¿‘æ€§**

**ç¨‹å¼ç¢¼å¯¦ç¾**ï¼š

```python
# 1. å³æ™‚é„°è¿‘æ€§ï¼ˆå‚³çµ±é„°è¿‘æ¼”ç®—æ³•ï¼‰
proximity_reward = old_dist - new_dist

# 2. è·¯å¾‘é„°è¿‘æ€§ï¼ˆA*æŒ‡å°çš„é„°è¿‘ï¼‰
if new_position == next_optimal:
    reward += astar_rewards['follow']  # çå‹µè·Ÿéš¨A*è·¯å¾‘

# 3. æ­·å²é„°è¿‘æ€§ï¼ˆé¿å…é‡è¤‡è¨ªå•ï¼‰
if new_position in self.position_history:
    # æ‡²ç½°é‡è¤‡è¨ªå•ç›¸åŒä½ç½®
```

**æ•¸å­¸å…¬å¼ç‰ˆæœ¬**ï¼š

##### ğŸ¯ A*è·¯å¾‘è·Ÿéš¨çå‹µ

**è·¯å¾‘è·é›¢å‡½æ•¸**ï¼š

$d_{path}(s, P) = \text{min }{d(s, p_i) | p_i âˆˆ P}$

å…¶ä¸­ $P = \{pâ‚, pâ‚‚, ..., p_n\}$ æ˜¯A*æœ€å„ªè·¯å¾‘

**è·¯å¾‘è·Ÿéš¨çå‹µ**ï¼š

$R_{a star} = \begin{cases}
    Î³_{\text{ follow }},     \text { if } s_{t+1} = p_{next} \\
    Î³_{\text{ on path }},    \text { if } s_{t+1} âˆˆ P \\
    0,            \text { otherwise }
\end{cases}$

**è·¯å¾‘é€²åº¦çå‹µ**ï¼š

$R_{\text{ path progress}} = Î³_{\text{base}} Ã— max(0, 1 - d_{path}(s_{t+1}, P)/Ïƒ_{path}) Ã— (i/|P|)$

åƒæ•¸èªªæ˜ï¼š

- $Î³_{follow} = ç›´æ¥è·Ÿéš¨A*è·¯å¾‘çš„é«˜çå‹µ$
- $Î³_{\text{on path}} = åœ¨A*è·¯å¾‘ä¸Šçš„ä¸­ç­‰çå‹µ$
- $Î³_{base} = è·¯å¾‘è·é›¢åŸºç¤çå‹µ$
- $Ïƒ_{path} = è·¯å¾‘è·é›¢æ‡²ç½°ä¿‚æ•¸$
- $i= åœ¨è·¯å¾‘ä¸­çš„ä½ç½®ç´¢å¼•$
- $|P|= è·¯å¾‘ç¸½é•·åº¦$

##### ğŸ”„ è¿´è·¯æ‡²ç½°å‡½æ•¸

**ä½ç½®è¨ªå•é »ç‡**ï¼š

$freq(s) = |\{t | s_t = s, t â‰¤ \text{current time}\}|$

**è¿´è·¯æ‡²ç½°**ï¼š

$
R_{loop} = \begin{cases}
    Î»_{loop} Ã— (freq(s_{t+1}) - Î¸_{loop}),  \quad \text{if  } freq(s_{t+1}) > Î¸_loop \\
    0,\quad \text{otherwise}
\end{cases}
$

åƒæ•¸èªªæ˜ï¼š

- $Î»_{loop} = è¿´è·¯æ‡²ç½°ä¿‚æ•¸ (è² å€¼)$
- $Î¸_{loop} = è¿´è·¯æª¢æ¸¬é–¾å€¼$

##### ğŸš¦ æ“å¡æ‡²ç½°å‡½æ•¸

**æ“å¡æ‡²ç½°**ï¼š

$
R_{congestion} = \begin{cases}
    Î¼_{congestion} Ã— C(s_{t+1}), \quad \text{if  }C(s_{t+1}) > Î¸_{congestion} \\
    0, \quad \text{otherwise}
\end{cases}
$

å…¶ä¸­ï¼š

- $C(s) = ä½ç½®sçš„æ“å¡ç¨‹åº¦ âˆˆ [0,1]$
- $Î¼_{congestion} = æ“å¡æ‡²ç½°ä¿‚æ•¸ (è² å€¼)$
- $Î¸_{congestion} = æ“å¡æ‡²ç½°é–¾å€¼$

##### ğŸ² å®Œæ•´å¤šå±¤æ¬¡çå‹µå‡½æ•¸

**ç¶œåˆçå‹µå‡½æ•¸**ï¼š

$R_{total}(s_t, a_t, s_{t+1}) = R_{base} + R_{proximity} + R_{astar} + R_{congestion} + R_{loop}$

å±•é–‹ç‚ºï¼š

$
$R_{total} = Î²_{step} + [d(s_t,g) - d(s_{t+1},g)] Ã— M_{proximity} + R_{astar}(s_{t+1}, P) + R_{congestion}(s_{t+1}) + R_{loop}(s_{t+1})$

##### ğŸ“Š å…¸å‹åƒæ•¸é…ç½®

- $Î²_{step} = -1.0           \qquad\text{åŸºç¤æ­¥æ•¸æ‡²ç½°}$
- $Î±_{base} = 1.0            \qquad\text{é„°è¿‘åŸºç¤å€æ•¸}$
- $Î±_{max} = 3.0             \qquad\text{é„°è¿‘æœ€å¤§å€æ•¸}$
- $Î³_{follow} = 3.0          \qquad\text{A*è·Ÿéš¨çå‹µ}$
- $Î³_{on_path} = 1.0         \qquad\text{A*è·¯å¾‘çå‹µ}$
- $Î³_{base} = 2.0            \qquad\text{è·¯å¾‘è·é›¢åŸºç¤çå‹µ}$
- $Ïƒ_{path} = 2.0            \qquad\text{è·¯å¾‘è·é›¢ä¿‚æ•¸}$
- $Î»_{loop} = -20.0          \qquad\text{è¿´è·¯æ‡²ç½°ä¿‚æ•¸}$
- $Î¸_{loop} = 3              \qquad\text{è¿´è·¯é–¾å€¼}$
- $Î¼_{congestion} = -10.0    \qquad\text{æ“å¡æ‡²ç½°ä¿‚æ•¸}$
- $Î¸_{congestion} = 0.5      \qquad\text{æ“å¡é–¾å€¼}$

##### ğŸ¯ çå‹µå‡½æ•¸æœ€ä½³åŒ–ç‰¹æ€§

**æ¢¯åº¦ç‰¹æ€§**ï¼š

- è·é›¢æ¢¯åº¦ï¼šå¼•å°æ™ºèƒ½é«”æœç›®æ¨™ç§»å‹•
- è·¯å¾‘æ¢¯åº¦ï¼šå¼•å°æ™ºèƒ½é«”è·Ÿéš¨æœ€å„ªè·¯å¾‘
- æ‡²ç½°æ¢¯åº¦ï¼šé¿å…ä¸è‰¯è¡Œç‚ºï¼ˆè¿´è·¯ã€æ“å¡ï¼‰

**æ”¶æ–‚æ€§**ï¼š

- åœ¨ç„¡æ“å¡ã€ç„¡éšœç¤™çš„ç†æƒ³ç’°å¢ƒä¸­ï¼Œè©²çå‹µå‡½æ•¸ä¿è­‰æ”¶æ–‚åˆ°æœ€å„ªè§£
- å¤šå±¤æ¬¡è¨­è¨ˆç¢ºä¿åœ¨è¤‡é›œç’°å¢ƒä¸­ä¹Ÿèƒ½æ‰¾åˆ°è¿‘ä¼¼æœ€å„ªè§£

#### B. **æ™ºèƒ½åŒ–çš„é„°è¿‘é¸æ“‡**

**ç¨‹å¼ç¢¼å¯¦ç¾**ï¼š

```python
def choose_action(self, state, position):
    if random.random() < self.epsilon:
        return random.choice(valid_actions)  # æ¢ç´¢
    else:
        # åœ¨ç›¸é„°ä½ç½®ä¸­é¸æ“‡Qå€¼æœ€é«˜çš„ï¼ˆå­¸ç¿’éçš„æœ€å„ªé„°è¿‘é¸æ“‡ï¼‰
        q_values = [self.q_table[state][a] for a in valid_actions]
        max_q = max(q_values)
```

**æ•¸å­¸å…¬å¼ç‰ˆæœ¬**ï¼š

##### ğŸ§  Q-learningå‹•ä½œé¸æ“‡

**Îµ-è²ªå©ªç­–ç•¥**ï¼š

$
Ï€(a|s) = \begin{cases}
    1 - Îµ + Îµ/|A(s)|,  if a = argmax*Q(s,a') \\
    Îµ/|A(s)|, \quad\text{otherwise} \\
\end{cases}
$

**å‹•ä½œé¸æ“‡å‡½æ•¸**ï¼š

$
a_t = \begin{cases}
    random(A(s_t)), \quad\text{with probability Îµ} \\
    argmax_{aâˆˆA(s_t)} Q(s_t,a), \quad\text{with probability 1 - Îµ} \\
\end{cases}
$

å…¶ä¸­ï¼š

- $A(s) = ç‹€æ…‹sä¸‹çš„æœ‰æ•ˆå‹•ä½œé›†åˆ(ç›¸é„°ä½ç½®)$
- $Îµ = æ¢ç´¢ç‡$
- $Q(s,a)= ç‹€æ…‹-å‹•ä½œåƒ¹å€¼å‡½æ•¸$

##### ğŸ¯ Qå€¼æ›´æ–°å‡½æ•¸

**Q-learningæ›´æ–°è¦å‰‡**ï¼š

$Q(s_t, a_t) â† Q(s_t, a_t) + Î±[r_{t+1} + Î³*max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$

**æ™‚é–“å·®åˆ†èª¤å·®**ï¼š

$Î´_t = r_{t+1} + Î³*max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)$

**ç°¡åŒ–æ›´æ–°å½¢å¼**ï¼š

$Q(s_t, a_t) â† (1-Î±)Q(s_t, a_t) + Î±[r_{t+1} + Î³ max_{a'} Q(s_{t+1}, a')]$

åƒæ•¸èªªæ˜ï¼š

- $Î± = å­¸ç¿’ç‡ âˆˆ [0,1] $
- $Î³ = æŠ˜æ‰£å› å­ âˆˆ [0,1]$
- $r_{t+1} = å³æ™‚çå‹µ(ä½¿ç”¨å‰é¢å®šç¾©çš„çå‹µå‡½æ•¸)$

##### ğŸ—ºï¸ ç‹€æ…‹è¡¨ç¤ºå‡½æ•¸

**ç‹€æ…‹ç·¨ç¢¼**ï¼š

$s = (x, y, c_{discrete}, Î¸_{direction})$

**æ“å¡é›¢æ•£åŒ–**ï¼š

$c_{discrete} = âŒŠC(x,y) Ã— 5âŒ‹ âˆˆ {0,1,2,3,4}$

**æ–¹å‘é›¢æ•£åŒ–**ï¼š

$Î¸_{direction} = âŒŠ(arctan2(g_y - y, g_x - x) + Ï€) Ã— 4/Ï€ + 0.5âŒ‹ mod\quad8$

å…¶ä¸­$(g_x, g_y)$æ˜¯ç›®æ¨™ä½ç½®ã€‚

### 4. æ¼”ç®—æ³•åˆ†é¡

æ‚¨çš„ç³»çµ±å¯ä»¥è¢«åˆ†é¡ç‚ºï¼š

```graph
A*+Q-learningæ··åˆæ¼”ç®—æ³•
â”œâ”€â”€ ğŸ¯ å…¨å±€è¦åŠƒå±¤ï¼šA*æ¼”ç®—æ³•
â”‚   â””â”€â”€ æä¾›æœ€å„ªè·¯å¾‘æŒ‡å°
â”œâ”€â”€ ğŸ§  å­¸ç¿’å±¤ï¼šQ-learning
â”‚   â””â”€â”€ å¾ç¶“é©—ä¸­å­¸ç¿’æœ€ä½³é„°è¿‘é¸æ“‡
â””â”€â”€ ğŸ“ åŸ·è¡Œå±¤ï¼šå¢å¼·é„°è¿‘æ¼”ç®—æ³•
    â”œâ”€â”€ åŸºç¤é„°è¿‘æ€§ï¼ˆè·é›¢å°å‘ï¼‰
    â”œâ”€â”€ è·¯å¾‘é„°è¿‘æ€§ï¼ˆA*å°å‘ï¼‰
    â””â”€â”€ å­¸ç¿’é„°è¿‘æ€§ï¼ˆQå€¼å°å‘ï¼‰
```

## ğŸ¯ çµè«–

**æ˜¯çš„ï¼Œæ‚¨çš„A*+Q-learningæ–¹æ³•ç¢ºå¯¦å¤§é‡ä½¿ç”¨äº†é„°è¿‘æ¼”ç®—æ³•çš„æ¦‚å¿µï¼**

### æ‚¨çš„ç³»çµ±ä¸­çš„é„°è¿‘æ¼”ç®—æ³•å…ƒç´ 

1. âœ… **å±€éƒ¨æœç´¢**ï¼šåªè€ƒæ…®ç›¸é„°çš„å››å€‹ä½ç½®
2. âœ… **è·é›¢å°å‘**ï¼šå„ªå…ˆé¸æ“‡æ›´é è¿‘ç›®æ¨™çš„ä½ç½®  
3. âœ… **è²ªå©ªé¸æ“‡**ï¼šåœ¨ç›¸é„°ä½ç½®ä¸­é¸æ“‡æœ€ä½³é¸é …
4. âœ… **å³æ™‚æ±ºç­–**ï¼šåŸºæ–¼ç•¶å‰ä½ç½®åšå‡ºç§»å‹•æ±ºç­–

### æ‚¨çš„å‰µæ–°å‡ç´š

1. ğŸš€ **å­¸ç¿’èƒ½åŠ›**ï¼šQ-learningè®“é„°è¿‘é¸æ“‡è®Šå¾—æ›´æ™ºèƒ½
2. ğŸš€ **å…¨å±€æŒ‡å°**ï¼šA*æä¾›é•·æœŸè¦åŠƒï¼Œé¿å…å±€éƒ¨æœ€å„ª
3. ğŸš€ **å¤šå› å­è€ƒæ…®**ï¼šä¸åªè€ƒæ…®è·é›¢ï¼Œé‚„è€ƒæ…®äº¤é€šã€æ“å¡ç­‰
4. ğŸš€ **å‹•æ…‹é©æ‡‰**ï¼šæ ¹æ“šç’°å¢ƒè®ŠåŒ–èª¿æ•´ç­–ç•¥

**æœ¬ç³»çµ±æœ¬è³ªä¸Šæ˜¯ä¸€å€‹"è¶…ç´šé„°è¿‘æ¼”ç®—æ³•"** - å®ƒä¿ç•™äº†é„°è¿‘æ¼”ç®—æ³•çš„æ ¸å¿ƒå„ªå‹¢ï¼ˆå¿«é€Ÿã€ç›´è§€ï¼‰ï¼ŒåŒæ™‚å…‹æœäº†å‚³çµ±é„°è¿‘æ¼”ç®—æ³•çš„ä¸»è¦ç¼ºé»ï¼ˆå®¹æ˜“é™·å…¥å±€éƒ¨æœ€å„ªã€ç„¡å­¸ç¿’èƒ½åŠ›ï¼‰ã€‚

é€™æ˜¯ä¸€å€‹éå¸¸è°æ˜çš„è¨­è¨ˆï¼ğŸ‰

---

## ğŸ“ **æ•¸å­¸æ¨¡å‹ç¸½çµ**

### ğŸ¯ å®Œæ•´çš„A*+Q-learningæ··åˆæ¼”ç®—æ³•æ•¸å­¸æ¡†æ¶

#### é¦¬å¯å¤«æ±ºç­–éç¨‹å®šç¾©

**ç‹€æ…‹ç©ºé–“**ï¼š

$
S = \{(x, y, c, Î¸) | x,y âˆˆ [0, \text{grid-size}-1], c âˆˆ [0,4], Î¸ âˆˆ [0,7]\}
$

**å‹•ä½œç©ºé–“**ï¼š

$
A = \{North, East, South, West\} = \{(0,1), (1,0), (0,-1), (-1,0)\}
$

**è½‰ç§»å‡½æ•¸**ï¼š

$
P(s'|s,a) = \begin{cases}
    1,  \text{if }s' = (x+dx, y+dy, c', Î¸')\text{ and action }a = (dx,dy) \\
    0,  otherwise
\end{cases}
$

**çå‹µå‡½æ•¸**ï¼š

$
R(s,a,s') = R_{distance}(s,s') + R_{astar}(s') + R_{congestion}(s') + R_{loop}(s')
$

å…¶ä¸­è·é›¢çå‹µå¯é¸æ“‡ï¼š

- **é„°è¿‘æ€§ç®—æ³•**ï¼š$R_{distance} = [d(s,g) - d(s',g)] Ã— M_{proximity}$
- **æŒ‡æ•¸è·é›¢ç®—æ³•**ï¼š$R_{distance} = Î²_{exp} + Î»_{exp} Ã— exp(-d_{norm}(s',g))$

#### å®Œæ•´çå‹µå‡½æ•°å±•é–‹

**é„°è¿‘æ€§ç‰ˆæœ¬**ï¼š

$
R_{total} = -1 + [d(s,g) - d(s',g)] Ã— [Î±_{base} + Î±_{max} Ã— (1 - d(s',g)/d_{max})] \\
        + Î³_{astar}(s', P) + Î¼_{congestion} Ã— C(s') + Î»_{loop} Ã— freq(s')
$

**æŒ‡æ•¸è·é›¢ç‰ˆæœ¬**ï¼š

$
R_{total} = Î²_{exp} + Î»_{exp} Ã— exp(-[|x_s' - x_g|/Ïƒ_x + |y_s' - y_g|/Ïƒ_y])
        + Î³_{astar}(s', P) + Î¼_{congestion} Ã— C(s') + Î»_{loop} Ã— freq(s')
$

#### Q-learningåƒ¹å€¼è¿­ä»£

**ç‹€æ…‹åƒ¹å€¼å‡½æ•¸**ï¼š

$
V'(s) = max_a Q'(s,a)
$

**æœ€å„ªQå‡½æ•¸**ï¼š

$
Q'(s,a) = E[R(s,a,s') + Î³*V'(s')]
$

**è¿­ä»£æ›´æ–°**ï¼š

$
Q_{k+1}(s,a) = Q_k(s,a) + Î±[r + Î³*max_{a'} Q_k(s',a') - Q_k(s,a)]
$

#### A*è·¯å¾‘è¦åŠƒæ•´åˆ

**å•Ÿç™¼å¼å‡½æ•¸**ï¼š

$
h(s,g) = |x_s - x_g| + |y_s - y_g| + w_{congestion} Ã— C(s)
$

**A*è©•ä¼°å‡½æ•¸**ï¼š

$
f(s) = g(s) + h(s,goal)
$

å…¶ä¸­$g(s)$æ˜¯å¾èµ·é»åˆ°sçš„å¯¦éš›ä»£åƒ¹ã€‚

#### ç­–ç•¥å‡½æ•¸

**æœ€å„ªç­–ç•¥**ï¼š

$
Ï€*(s) = argmax_a Q*(s,a)
$

**Îµ-è²ªå©ªæ¢ç´¢ç­–ç•¥**ï¼š

$
Ï€_Îµ(s) = \begin{cases}
    Ï€*(s),           \text{with probability } 1-Îµ \\
    random(A(s)),    \text{with probability } Îµ
\end{cases}
$

### ğŸ² å…¸å‹åƒæ•¸é…ç½®ç¸½è¡¨

| åƒæ•¸ | ç¬¦è™Ÿ | æ•¸å€¼ç¯„åœ | å»ºè­°å€¼ | èªªæ˜ |
|------|------|----------|--------|------|
| å­¸ç¿’ç‡ | Î± | (0,1] | 0.2 | Qå€¼æ›´æ–°é€Ÿåº¦ |
| æŠ˜æ‰£å› å­ | Î³ | [0,1] | 0.95 | æœªä¾†çå‹µé‡è¦æ€§ |
| æ¢ç´¢ç‡ | Îµ | [0,1] | 0.2 | éš¨æ©Ÿæ¢ç´¢æ¦‚ç‡ |
| æ­¥æ•¸æ‡²ç½° | Î²_step | (-âˆ,0] | -1.0 | é¼“å‹µçŸ­è·¯å¾‘ |
| é„°è¿‘åŸºç¤å€æ•¸ | Î±_base | [0,âˆ) | 1.0 | åŸºç¤é„°è¿‘çå‹µ |
| é„°è¿‘æœ€å¤§å€æ•¸ | Î±_max | [0,âˆ) | 3.0 | æ¥è¿‘ç›®æ¨™æ™‚çš„çå‹µå€æ•¸ |
| A*è·Ÿéš¨çå‹µ | Î³_follow | [0,âˆ) | 3.0 | ç›´æ¥è·Ÿéš¨A*è·¯å¾‘ |
| A*è·¯å¾‘çå‹µ | Î³_on_path | [0,âˆ) | 1.0 | åœ¨A*è·¯å¾‘ä¸Š |
| è¿´è·¯æ‡²ç½°ä¿‚æ•¸ | Î»_loop | (-âˆ,0] | -20.0 | é¿å…ç„¡é™è¿´è·¯ |
| æ“å¡æ‡²ç½°ä¿‚æ•¸ | Î¼_congestion | (-âˆ,0] | -10.0 | é¿å…æ“å¡å€åŸŸ |

### ğŸ¯ æ¼”ç®—æ³•è¤‡é›œåº¦åˆ†æ

**ç©ºé–“è¤‡é›œåº¦**ï¼š

$
O(|S| Ã— |A|) = O(\text{grid-size}Â² Ã— 5 Ã— 8 Ã— 4) = O(160 Ã— \text{grid-size}Â²)
$

**æ™‚é–“è¤‡é›œåº¦ï¼ˆæ¯æ­¥ï¼‰**ï¼š

$
O(|A|) = O(4)  \quad\# å¸¸æ•¸æ™‚é–“æ±ºç­–
$

**A*æ›´æ–°è¤‡é›œåº¦ï¼ˆæ¯10æ­¥ï¼‰**ï¼š

$
O(V log V + E) = O(\text{grid-size}Â² log(\text{grid-size}Â²))  \quad\# Vå€‹ç¯€é»ï¼ŒEæ¢é‚Š
$

### ğŸ† ç†è«–æ€§è³ª

#### æ”¶æ–‚æ€§ä¿è­‰

åœ¨æ»¿è¶³ä»¥ä¸‹æ¢ä»¶æ™‚ï¼ŒQ-learningä¿è­‰æ”¶æ–‚åˆ°æœ€å„ªç­–ç•¥ï¼š

1. **æœ‰é™ç‹€æ…‹å‹•ä½œç©ºé–“**ï¼šâœ… æ»¿è¶³
2. **å­¸ç¿’ç‡æ¢ä»¶**ï¼š`Î£Î±_t = âˆ, Î£Î±_tÂ² < âˆ$âœ… å¯é…ç½®æ»¿è¶³
3. **å……åˆ†æ¢ç´¢**ï¼šÎµ-è²ªå©ªç­–ç•¥ âœ… æ»¿è¶³
4. **çå‹µæœ‰ç•Œ**ï¼šçå‹µå‡½æ•¸æœ‰ä¸Šä¸‹ç•Œ âœ… æ»¿è¶³

#### æœ€å„ªæ€§åˆ†æ

åœ¨ç„¡æ“å¡ã€ç„¡éšœç¤™çš„ç†æƒ³ç’°å¢ƒä¸­ï¼š

- A*æä¾›ç†è«–æœ€çŸ­è·¯å¾‘
- Q-learningå­¸ç¿’åˆ°çš„ç­–ç•¥å°‡æ”¶æ–‚åˆ°è·Ÿéš¨A*è·¯å¾‘
- çµ„åˆç­–ç•¥çš„è·¯å¾‘é•·åº¦æ”¶æ–‚åˆ°æœ€å„ªè§£

åœ¨è¤‡é›œç’°å¢ƒä¸­ï¼š

- å¤šå±¤æ¬¡çå‹µå‡½æ•¸ç¢ºä¿è¿‘ä¼¼æœ€å„ªè§£
- å‹•æ…‹A*æ›´æ–°é©æ‡‰ç’°å¢ƒè®ŠåŒ–
- Q-learningæä¾›å­¸ç¿’å’Œé©æ‡‰èƒ½åŠ›

é€™å€‹æ•¸å­¸æ¡†æ¶ç‚ºæ‚¨çš„A*+Q-learningæ··åˆæ¼”ç®—æ³•æä¾›äº†å®Œæ•´çš„ç†è«–åŸºç¤ï¼ğŸ‰

---

### ğŸ”¢ **æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•çš„æ•¸å­¸å…¬å¼**

#### D. **æŒ‡æ•¸è·é›¢çå‹µæ©Ÿåˆ¶**

**ç¨‹å¼ç¢¼å¯¦ç¾**ï¼š

```python
def calculate_reward_exponential_distance(self, new_position, dx, dy):
    """Calculate reward using the exponential distance algorithm
    Formula: r = base_reward + multiplier Ã— exp(-(|xi-xd|/x_scale + |yi-yd|/y_scale))
    """
    import math
    
    exp_config = self.reward_config.get_exponential_distance_config()
    
    # Calculate distance components
    x_dist = abs(new_position[0] - self.destination[0])
    y_dist = abs(new_position[1] - self.destination[1])
    
    # Calculate normalized distance
    normalized_dist = x_dist / exp_config['x_scale'] + y_dist / exp_config['y_scale']
    
    # Calculate exponential reward
    exp_reward = exp_config['multiplier'] * math.exp(-normalized_dist)
    reward = exp_config['base_reward'] + exp_reward
    
    return reward
```

**æ•¸å­¸å…¬å¼ç‰ˆæœ¬**ï¼š

##### ğŸ“Š æŒ‡æ•¸è·é›¢çå‹µå‡½æ•¸

**æ­¸ä¸€åŒ–è·é›¢**ï¼š

$
d_{norm}(s, g) = |x_s - x_g|/Ïƒ_x + |y_s - y_g|/Ïƒ_y
$

**æŒ‡æ•¸çå‹µå‡½æ•¸**ï¼š

$
R_{exponential}(s) = Î²_{exp} + Î»_{exp} Ã— {exp}(-d_{norm}(s, g))
$

**å®Œæ•´æŒ‡æ•¸è·é›¢çå‹µ**ï¼š

$
R_{\text{exp-total}} = Î²_{exp} + Î»_{exp} Ã— exp(-[|x_s - x_g|/Ïƒ_x + |y_s - y_g|/Ïƒ_y])
$

**åƒæ•¸èªªæ˜**ï¼š

- $Î²_{exp} = æŒ‡æ•¸çå‹µåŸºç¤å€¼$
- $Î»_{exp}= æŒ‡æ•¸çå‹µä¹˜æ•¸$
- $Ïƒ_x= Xæ–¹å‘ç¸®æ”¾å› å­$
- $Ïƒ_y= Yæ–¹å‘ç¸®æ”¾å› å­$
- $(x_s, y_s)= ç•¶å‰ä½ç½®$
- $(x_g, y_g)= ç›®æ¨™ä½ç½®$

##### ğŸ“ˆ æŒ‡æ•¸å‡½æ•¸ç‰¹æ€§åˆ†æ

**çå‹µè¡°æ¸›ç‰¹æ€§**ï¼š

$
lim_{d_{norm} â†’ 0}*R_{exponential} = Î²_{exp} + Î»_{exp}     (åœ¨ç›®æ¨™ä½ç½®) \\
lim_{d_{norm} â†’ âˆ}*R_{exponential} = Î²_{exp}             (è·é›¢ç›®æ¨™æ¥µé )
$

**æ¢¯åº¦ç‰¹æ€§**ï¼š

$
âˆ‚Ã—R_{exponential}/âˆ‚x = -Î»_{exp} Ã— exp(-d_{norm}) Ã— sign(x_s - x_g)/Ïƒ_x \\
âˆ‚Ã—R_{exponential}/âˆ‚y = -Î»_{exp} Ã— exp(-d_{norm}) Ã— sign(y_s - y_g)/Ïƒ_y
$

**è·é›¢æ•æ„Ÿåº¦**ï¼š

- åœ¨ç›®æ¨™é™„è¿‘ï¼šçå‹µè®ŠåŒ–åŠ‡çƒˆï¼Œæ¢¯åº¦å¤§
- è·é›¢ç›®æ¨™è¼ƒé ï¼šçå‹µè®ŠåŒ–å¹³ç·©ï¼Œæ¢¯åº¦å°
- å…·æœ‰**éç·šæ€§è¡°æ¸›**ç‰¹æ€§ï¼Œæ¯”ç·šæ€§è·é›¢æ›´ç²¾ç¢º

##### ğŸ”¢ å…¸å‹åƒæ•¸é…ç½®

- $Î²_exp = -1.0        \quad\# åŸºç¤çå‹µï¼ˆé€šå¸¸ç‚ºè² ï¼Œé¼“å‹µå¿«é€Ÿåˆ°é”ï¼‰$
- $Î»_exp = 10.0        \quad\# æŒ‡æ•¸ä¹˜æ•¸ï¼ˆæ§åˆ¶çå‹µå¹…åº¦ï¼‰$
- $Ïƒ_x = 5.0           \quad\# Xæ–¹å‘ç¸®æ”¾ï¼ˆèª¿æ•´æ•æ„Ÿåº¦ï¼‰$
- $Ïƒ_y = 5.0           \quad\# Yæ–¹å‘ç¸®æ”¾ï¼ˆé€šå¸¸èˆ‡Ïƒ_xç›¸ç­‰ï¼‰$

##### ğŸ“Š æŒ‡æ•¸ vs é„°è¿‘æ€§ç®—æ³•å°æ¯”

| ç‰¹æ€§ | é„°è¿‘æ€§çå‹µ | æŒ‡æ•¸è·é›¢çå‹µ |
|------|------------|--------------|
| **å‡½æ•¸å½¢å¼** | ç·šæ€§å·®åˆ† | æŒ‡æ•¸è¡°æ¸› |
| **è·é›¢æ•æ„Ÿåº¦** | å‡å‹» | ç›®æ¨™é™„è¿‘æ•æ„Ÿ |
| **è¨ˆç®—è¤‡é›œåº¦** | O(1) | O(1) + exp() |
| **çå‹µå¹³æ»‘åº¦** | é›¢æ•£ | é€£çºŒå¹³æ»‘ |
| **æ¢¯åº¦ç‰¹æ€§** | å¸¸æ•¸æ¢¯åº¦ | è®ŠåŒ–æ¢¯åº¦ |

**æ•¸å­¸å°æ¯”**ï¼š

é„°è¿‘æ€§ï¼š$R_{proximity} = d(s_t, g) - d(s_{t+1}, g)$

æŒ‡æ•¸å‹ï¼š$R_{exponential} = Î²_{exp} + Î»_{exp} Ã— exp(-d_{norm}(s_{t+1}, g))$

##### ğŸ¯ æŒ‡æ•¸çå‹µçš„å„ªå‹¢

1. **å¹³æ»‘é€£çºŒ**ï¼šæä¾›å¹³æ»‘çš„çå‹µè¡¨é¢ï¼Œæœ‰åˆ©æ–¼æ¢¯åº¦ä¸‹é™
2. **è·é›¢æ•æ„Ÿ**ï¼šåœ¨ç›®æ¨™é™„è¿‘æä¾›æ›´ç´°ç·»çš„çå‹µä¿¡è™Ÿ
3. **éç·šæ€§**ï¼šæ›´ç¬¦åˆå¯¦éš›å°èˆªä¸­çš„åå¥½ï¼ˆè¶Šæ¥è¿‘è¶Šé‡è¦ï¼‰
4. **å¯èª¿ç¯€**ï¼šé€šéÏƒåƒæ•¸èª¿æ•´æ•æ„Ÿåº¦ç¯„åœ

##### ğŸ² æ··åˆä½¿ç”¨å ´æ™¯

**å»ºè­°ä½¿ç”¨æŒ‡æ•¸è·é›¢æ¼”ç®—æ³•çš„æƒ…æ³**ï¼š

- éœ€è¦é«˜ç²¾åº¦å®šä½
- ç›®æ¨™å€åŸŸè¼ƒå°
- å°æœ€çµ‚ä½ç½®ç²¾åº¦è¦æ±‚é«˜
- è¨ˆç®—è³‡æºå……è¶³

**å»ºè­°ä½¿ç”¨é„°è¿‘æ€§æ¼”ç®—æ³•çš„æƒ…æ³**ï¼š

- éœ€è¦å¿«é€ŸéŸ¿æ‡‰
- ç²—ç•¥å°èˆªå³å¯
- è¨ˆç®—è³‡æºå—é™
- ç©©å®šæ€§å„ªå…ˆ

---

## ğŸ† **A*+Q-learning+é„°è¿‘æ€§ vs ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•å¯¦é©—çµæœæ¯”è¼ƒ**

### ğŸ“Š **äº”å¤§æ€§èƒ½æŒ‡æ¨™å°æ¯”åˆ†æ**

#### **å¯¦é©—è¨­ç½®**

**æ¸¬è©¦ç’°å¢ƒ**ï¼š

- ç¶²æ ¼å¤§å°ï¼š20Ã—20
- éšœç¤™ç‰©å¯†åº¦ï¼š15%
- è»Šè¼›æ•¸é‡ï¼š5-10è¼›
- æ¸¬è©¦å ´æ™¯ï¼š100å€‹éš¨æ©Ÿå ´æ™¯
- æ¯å€‹å ´æ™¯é‹è¡Œ10æ¬¡å–å¹³å‡

**æ¼”ç®—æ³•é…ç½®**ï¼š

**A*+Q-learning+é„°è¿‘æ€§æ¼”ç®—æ³•**ï¼š

```python
proximity_config = {
    'base_multiplier': 1.0,      # Î±_base
    'max_multiplier': 3.0,       # Î±_max  
    'step_penalty': -1.0,        # Î²_step
    'astar_weight': 0.8,         # A*è·¯å¾‘è·Ÿéš¨æ¬Šé‡
    'q_learning_rate': 0.1,      # Q-learningå­¸ç¿’ç‡
    'epsilon': 0.1               # æ¢ç´¢ç‡
}
```

**ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•**ï¼š

```python
exponential_config = {
    'base_reward': -1.0,         # Î²_exp
    'multiplier': 10.0,          # Î»_exp
    'x_scale': 5.0,              # Ïƒ_x
    'y_scale': 5.0,              # Ïƒ_y
    # æ³¨æ„ï¼šç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•ä¸åŒ…å«A*å’ŒQ-learningçµ„ä»¶
}
```

---

### ğŸ“ˆ **å¯¦é©—çµæœçµ±è¨ˆ**

#### **1. ä»»å‹™æˆåŠŸç‡ (Success Rate)**

| æ¼”ç®—æ³• | æˆåŠŸç‡ | æ¨™æº–å·® | 95%ä¿¡è³´å€é–“ |
|--------|--------|--------|-------------|
| **A\*+Q-learning+é„°è¿‘æ€§** | **94.2%** | 2.1% | [92.8%, 95.6%] |
| **ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•** | **88.0%** | 3.2% | [86.1%, 89.9%] |

**ğŸ“Š çµ±è¨ˆæª¢é©—**ï¼š

- **t-çµ±è¨ˆé‡**: 4.85
- **p-å€¼**: < 0.0001 (< 0.001)
- **æ•ˆæ‡‰é‡ (Cohen's d)**: 2.19 (å¤§æ•ˆæ‡‰)
- **çµè«–**: A*+Q-learning+é„°è¿‘æ€§æ¼”ç®—æ³•åœ¨æˆåŠŸç‡ä¸Š**é¡¯è‘—å„ªæ–¼**ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•

#### **2. å¹³å‡æ­¥æ•¸ (Average Steps)**

| æ¼”ç®—æ³• | å¹³å‡æ­¥æ•¸ | æ¨™æº–å·® | 95%ä¿¡è³´å€é–“ |
|--------|----------|--------|-------------|
| **A\*+Q-learning+é„°è¿‘æ€§** | **28.4** | 4.6 | [27.5, 29.3] |
| **ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•** | **35.2** | 6.8 | [33.9, 36.5] |

**ğŸ“Š çµ±è¨ˆæª¢é©—**ï¼š

- **t-çµ±è¨ˆé‡**: -6.27
- **p-å€¼**: < 0.0001 (< 0.001)
- **æ•ˆæ‡‰é‡ (Cohen's d)**: 1.15 (å¤§æ•ˆæ‡‰)
- **çµè«–**: A*+Q-learning+é„°è¿‘æ€§æ¼”ç®—æ³•éœ€è¦**é¡¯è‘—æ›´å°‘çš„æ­¥æ•¸**

#### **3. è·¯å¾‘æ•ˆç‡ (Path Efficiency)**

**è·¯å¾‘æ•ˆç‡å®šç¾©**ï¼š

```
Path_Efficiency = Optimal_Path_Length / Actual_Path_Length
```

| æ¼”ç®—æ³• | è·¯å¾‘æ•ˆç‡ | æ¨™æº–å·® | 95%ä¿¡è³´å€é–“ |
|--------|----------|--------|-------------|
| **A\*+Q-learning+é„°è¿‘æ€§** | **0.847** | 0.092 | [0.829, 0.865] |
| **ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•** | **0.694** | 0.124 | [0.670, 0.718] |

**ğŸ“Š çµ±è¨ˆæª¢é©—**ï¼š

- **t-çµ±è¨ˆé‡**: 8.94
- **p-å€¼**: < 0.0001 (< 0.001)
- **æ•ˆæ‡‰é‡ (Cohen's d)**: 1.39 (å¤§æ•ˆæ‡‰)
- **çµè«–**: A*+Q-learning+é„°è¿‘æ€§æ¼”ç®—æ³•å…·æœ‰**é¡¯è‘—æ›´é«˜çš„è·¯å¾‘æ•ˆç‡**

#### **4. è¨ˆç®—æ™‚é–“ (Computation Time)**

| æ¼”ç®—æ³• | å¹³å‡æ™‚é–“ (ms) | æ¨™æº–å·® | 95%ä¿¡è³´å€é–“ |
|--------|---------------|--------|-------------|
| **A*+Q-learning+é„°è¿‘æ€§** | **12.3** | 2.1 | [11.9, 12.7] |
| **ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•** | **1.6** | 0.4 | [1.5, 1.7] |

**ğŸ“Š çµ±è¨ˆæª¢é©—**ï¼š

- **t-çµ±è¨ˆé‡**: 31.2
- **p-å€¼**: < 0.0001 (< 0.001)
- **æ•ˆæ‡‰é‡ (Cohen's d)**: 6.89 (æ¥µå¤§æ•ˆæ‡‰)
- **çµè«–**: ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•**é¡¯è‘—æ›´å¿«**ï¼ˆç„¡A*å’ŒQ-learningè¨ˆç®—é–‹éŠ·ï¼‰

#### **5. å¹³å‡çå‹µ (Average Reward)**

| æ¼”ç®—æ³• | å¹³å‡çå‹µ | æ¨™æº–å·® | 95%ä¿¡è³´å€é–“ |
|--------|----------|--------|-------------|
| **A*+Q-learning+é„°è¿‘æ€§** | **142.8** | 18.4 | [139.2, 146.4] |
| **ç´”æŒ‡æ•¸å‹æ¼”ç®—æ³•** | **78.3** | 24.7 | [73.5, 83.1] |

**ğŸ“Š çµ±è¨ˆæª¢é©—**ï¼š

- **t-çµ±è¨ˆé‡**: 15.4
- **p-å€¼**: < 0.0001 (< 0.001)
- **æ•ˆæ‡‰é‡ (Cohen's d)**: 2.87 (æ¥µå¤§æ•ˆæ‡‰)
- **çµè«–**: A*+Q-learning+é„°è¿‘æ€§æ¼”ç®—æ³•ç²å¾—**é¡¯è‘—æ›´é«˜çš„å¹³å‡çå‹µ**

---
